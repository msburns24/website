[
  {
    "objectID": "projects/titanic.html",
    "href": "projects/titanic.html",
    "title": "Titanic",
    "section": "",
    "text": "To most, the Titanic may evokes imagery of a grand ship, a newspaper headline, or maybe the iconic picture of Leo & Kate on the bow of the ship. To data scientists, the Titanic often reminds us of our first dataset, a real dataset, requiring vast amounts of cleaning, preprocessing, transformation, and hyperparameter tuning.\nNevertheless, the Titanic dataset is a great place to start with data science projects. It is a great place to start, covering a great variety of topics."
  },
  {
    "objectID": "projects/titanic.html#setup",
    "href": "projects/titanic.html#setup",
    "title": "Titanic",
    "section": "Setup",
    "text": "Setup\n\nDownload Data\n\n\nDefine Workflow\nBefore we start, let’s define the scope of our workflow. I’m taking the steps from the framework CRISP-DM (Cross-Industry Standard Process for Data Mining). Data Science PM has a useful overview of this framework on their website.\n\nBusiness Understanding\nData Understanding\nData Preparation\nModeling\nEvaluation\nDeployment"
  },
  {
    "objectID": "projects/titanic.html#business-understanding",
    "href": "projects/titanic.html#business-understanding",
    "title": "Titanic",
    "section": "1. Business Understanding",
    "text": "1. Business Understanding\n\nBusiness Objectives\nTypically, we’ll want to thoroughly understand the business side of things before embarking on an analysis. This involves a lot of discussion with stakeholders–the ones who feel the most impact from the problem at hand.\nPerhaps we need an understanding of what went wrong on the Titanic in order to prevent future disasters. Or, maybe we want to analyze the most impactful features of passengers in their survival so as to determine the effect of society norms in the early 20th century on a passenger surviving any kind of disaster.\nRegardless of the business case, we’re looking to understand survival of Titanic passengers and create a model to predict which passengers will survive.\n\n\nAssess the Situation\nHere is where we would need to evaluate what resources we have available. If our model was computationally intensive, what resources does our company have with cloud computing? Are there other metadata or supplemental datasets that may be useful here?\nFor our purpose, we simply have the dataset provided to us by Kaggle. As we get further into the analysis, we can evaluate risks/contingencies based on the cleanliness of the data.\n\n\nDetermine the Goals\n\nIn short, what does success look like?\n\nPersonally, I would love to see a model that can predict (with excellent accuracy) which passengers will survive the Titanic. I anticipate that the bare features alone won’t cut it, so a successful model will likely have well-engineered features that make use of the raw data. Finally, a successful project would be one that considers multiple models, using cross-validation, and implements the most successful one.\nSummarizing these goals, we want to:\n\nCreate a machine learning model to predict which passengers survive\nHave excellent performance (to be clarified later)\nDevelop well-engineered, meaningful features\nUtilize cross-validation to select the best-performing model\n\n\n\nFormulate a Project Plan\nWhat kind of technologies do we want to use, and how will we implement them?\nWe’ll need to revisit this as we get further understanding of the data, but initially, we could make use of:\n\nLogistic Regression\nNaive-Bayes\nk-Nearest Neighbors\nDecision Tree\nRandom Forest\nGradient-Boosted Trees\nAda Boost\nSupport Vector Machines\n\nMuch of these are quickly available in the Scikit-Learn library. In addition, this library also provides tools for:\n\nTrain-Test splits\nK-fold cross-validation\nStratified K-Fold cross-validation\nGrid search CV\nAccuracy metrics"
  },
  {
    "objectID": "projects/titanic.html#data-understanding",
    "href": "projects/titanic.html#data-understanding",
    "title": "Titanic",
    "section": "2. Data Understanding",
    "text": "2. Data Understanding\nThis stage is more commonly referred to as EDA, or exploratory data analysis. We want to import our data and understand its structure, what problems we need to fix, and any other factors that may affect our workflow.\n\nLoad Data\n\n# Import required modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nimport seaborn as sns\n\n\ndf = pd.read_csv('data/Titanic-Dataset.csv')\nprint('Loaded data into variable `df`')\n\nLoaded data into variable `df`\n\n\n\n\nDescribe our Data\nHere, we want to understand some high-level characteristics of our data, namely:\n\nData preview (i.e., the head)\nData types\nDescriptive statistics\nNull value distribution\n\n\n# Display first 10 rows\ndf.head(10)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\n\nNotes:\n\nPassengerId looks like a simple incrementing ID; this should be set as the index.\nSurvived is our target, a one-hot variable with 1 = “Survived”\nPClass seems to be a numeric value indicating first, second, or third class rooms on the ship.\nName doesn’t just include first/last name, it also includes title. This may be useful for an engineered feature.\nSex seems to be only male/female – this should be converted to a one-hot variable.\nAge - I see one NaN value here. I also notice it got represented as a float – is there a passenger whose age isn’t a whole integer?\nSibSp - Not immediately clear, but this one represents the number of siblings or spouses on the Titanic. I.e., other people in their group of the same age.\nParch - Like SibSp, this isn’t immediately clear, but the documentation says this represents the number of parents/children in their group. Both this column and SibSp may be useful to quantify the total people in the party’s group.\nTicket - Some of these are integers (ticket number), but some also have information like ‘A/5’, ‘PC’, ‘STON/O2’. Will need to dig further to understand this field.\nFare - How much was paid for the ticket. Do higher-paying customers have better chance of survival?\nCabin - The cabin number. May be useful if Pclass isn’t available.\nEmbarked - S = Southampton, C = Cherbourg, Q = Queenstown. Note that the order here is not arbitrary: This is the order of ports that the Titanic visited.\n\n\n\n# Display data types\ndisplay(df.dtypes.value_counts().to_frame('# Columns'))\ndisplay(df.dtypes.to_frame(name='Type'))\n\n\n\n\n\n\n\n\n# Columns\n\n\n\n\nint64\n5\n\n\nobject\n5\n\n\nfloat64\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\n\n\nPassengerId\nint64\n\n\nSurvived\nint64\n\n\nPclass\nint64\n\n\nName\nobject\n\n\nSex\nobject\n\n\nAge\nfloat64\n\n\nSibSp\nint64\n\n\nParch\nint64\n\n\nTicket\nobject\n\n\nFare\nfloat64\n\n\nCabin\nobject\n\n\nEmbarked\nobject\n\n\n\n\n\n\n\n\nNotes:\nAs mentioned above, it’s interesting that the Age field came in as a float. It may make more sense to convert this to an integer, unless that partial year gives us some kind of information.\nAlso, while it may be tempting to get rid of the 5 string fields, we can’t disregard this yet – we don’t have a ton of information to work with, so we’ll want to extract as much information as possible.\n\n\n# Describe the data\ndf.describe().round(2)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.00\n891.00\n891.00\n714.00\n891.00\n891.00\n891.00\n\n\nmean\n446.00\n0.38\n2.31\n29.70\n0.52\n0.38\n32.20\n\n\nstd\n257.35\n0.49\n0.84\n14.53\n1.10\n0.81\n49.69\n\n\nmin\n1.00\n0.00\n1.00\n0.42\n0.00\n0.00\n0.00\n\n\n25%\n223.50\n0.00\n2.00\n20.12\n0.00\n0.00\n7.91\n\n\n50%\n446.00\n0.00\n3.00\n28.00\n0.00\n0.00\n14.45\n\n\n75%\n668.50\n1.00\n3.00\n38.00\n1.00\n0.00\n31.00\n\n\nmax\n891.00\n1.00\n3.00\n80.00\n8.00\n6.00\n512.33\n\n\n\n\n\n\n\n\nNotes:\n\nPassengerId - Irrelevant, as this is just an index.\nSurvived - The min/max don’t tell us much, but the mean is useful – 38% of passengers survived, so around 1 in 3.\nPclass - This is an ordinal field, not numeric, but it does tell us that the average passenger was between class 2 and 3.\nAge - There is a variety of ages on the Titanic, with the typical passenger being about 28-29 years of age, the youngest was 0.42 (5 months) and the eldest was 80 years old. Most passengers (25-75 percentile) were between 20-38 years old.\nSibSp - Most passengers (&gt;50%) didn’t have any siblings or spouses. Even the 75% percentile was just 1 other person of their age. The largest group was 8 siblings/spouses (almost certainly 8 siblings).\nParch - The vast majority (&gt;75%) didn’t have any parents or children aboard, but one group had 6 parents/children.\nFare - On average, passengers paid $32.20 with a wide standard deviation of $49.69. Most passengers paid between $8-31, with the wealthiest passenger paying a whopping $512.33 for their fare.\n\n\n\n# Understand the null distribution\nnull_distribution = pd.DataFrame({\n    'Null Count': df.isna().sum().map(lambda n: '' if n == 0 else f'{n:,}'),\n    '% Null': df.isna().mean().map(lambda p: '' if p == 0 else f'{p:.2%}')\n})\nnull_distribution = null_distribution.loc[null_distribution.iloc[:,0] != '']\n\nnull_distribution.T\n\n\n\n\n\n\n\n\nAge\nCabin\nEmbarked\n\n\n\n\nNull Count\n177\n687\n2\n\n\n% Null\n19.87%\n77.10%\n0.22%\n\n\n\n\n\n\n\n\nNotes:\nMost fields don’t have any null values, which is helpful. The most problematic field will be Cabin, but from looking at the top 10 rows, it doesn’t seem like this will be useful. We may just drop this field all together.\nAge is an interesting one: there is a significant amount of nulls here, but it may be quite a useful field. We could look at doing a quick predictive model based on Name, Fare, Embarked, SibSp, and Parch. Or, if we want something quick, we could do a simple median-value or mean-value imputation.\nEmbarked will probably require a quick manual fix here, since it’s only 2 records.\n\n\n\nExplore the Data\nNow for the fun part of EDA–the visuals! Let’s try to visualize what our data looks like. Each cell will be looking to answer a preliminary question or one raised in the data description step prior to this.\n\ndef add_bar_labels(\n        ax: Axes,\n        fmt: str,\n        label_type='center',\n        color='white'\n) -&gt; None:\n    for container in ax.containers:\n        ax.bar_label(container, label_type=label_type, fmt=fmt, color=color)\n    return\n\n\n# How did class play a role in survival?\ndf_class = df.groupby('Pclass', as_index=False)[['Survived']].mean()\nax = df_class.plot(\n    kind='bar',\n    x='Pclass',\n    y='Survived',\n    xlabel='Passenger Class',\n    ylabel='Passenger Survival (%)',\n    rot=0,\n    color='dodgerblue',\n    legend=False,\n    title='Passenger Survival by Class'\n)\n\n# X Labels\nax.set_xticks(range(3), ['First', 'Second', 'Third'])\n\n# Y Labels\nyticks = ax.get_yticks()\nytick_labels = map('{:.0%}'.format, yticks)\nax.set_yticks(yticks, ytick_labels)\n\nadd_bar_labels(ax, fmt='{:.1%}')\n\n# ax.set_title('Passenger Survival by Class', size=14, pad=12)\nplt.show()\n\n\n\n\n\n\n\n\n\nNotes:\nUnsurprisingly, first-class passengers has the best chance of surviving, followed by second, then third class.\n\n\n# What was the distribution of sex on the Titanic? How did this affect surival?\n\n# Create pivot-table by sex\ndf_sex = df.groupby(['Sex', 'Survived'], as_index=False)[['PassengerId']].count()\ndf_sex = df_sex.pivot(index='Sex', columns='Survived', values='PassengerId')\ndf_sex.rename(inplace=True, columns=lambda s: ['No', 'Yes'][s])\ndf_sex.sort_index(inplace=True, axis=1, ascending=False)  # Show 'Yes' first\n\n# Plot data\nax = df_sex.plot(\n    kind='bar',\n    stacked=True,\n    color=['dodgerblue', 'lightgray'],\n    legend=False,\n    title='Survival by Sex',\n    rot=0,\n)\nadd_bar_labels(ax, fmt='{:,.0f}', color='black')\nplt.show()\n\n\n\n\n\n\n\n\n\n# How does Age play an impact in survival?\ndf_age = df.groupby('Age', as_index=False)[['Survived']].mean()\ndf_age['Survived'] = df_age['Survived'].ewm(alpha=0.2).mean()\n\nax = df_age.plot(\n    kind='line',\n    x='Age',\n    y='Survived',\n    legend=False,\n    title='Survival by Age',\n    xlabel='Age',\n    ylabel='Survival (%)',\n    xlim=(0, 80),\n    ylim=(0, 1),\n)\n\nyticks = ax.get_yticks()\nytick_labels = map('{:.0%}'.format, yticks)\nax.set_yticks(yticks, ytick_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Did older passengers pay higher fares?\n\ndf_age_fare = df.groupby('Age', as_index=False)[['Fare']].mean()\ndf_age_fare['Fare'] = df_age_fare['Fare'].ewm(alpha=0.05).mean()\n\nax = df_age_fare.plot(\n    kind='line',\n    x='Age',\n    y='Fare',\n    legend=False,\n    title='Fare by Age',\n    xlabel='Age',\n    ylabel='Average Fare ($)',\n    xlim=(1, 80),\n    ylim=(0, 60),\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# How does Fare play an impact in survival?\ndf_fare = df.groupby('Fare', as_index=False)[['Survived']].mean()\ndf_fare['Survived'] = df_fare['Survived'].ewm(alpha=0.02).mean()\n\nax = df_fare.plot(\n    kind='line',\n    x='Fare',\n    y='Survived',\n    legend=False,\n    title='Survival by fare',\n    xlabel='Fare',\n    ylabel='Survival (%)',\n    xlim=(0, 500),\n    ylim=(0, 1),\n)\n\nyticks = ax.get_yticks()\nytick_labels = map('{:.0%}'.format, yticks)\nax.set_yticks(yticks, ytick_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\nNotes:\nIt seems that generally, higher fares meant higher chance of survival, but there was a small dip around $30-50 fare, where passnegers that paid around $20-30 had a slightly higher chance of survival.\n\n\n# What are the most common words within \"Name\"?\nis_alnum_or_space = lambda s: s.isalpha() or s == ' '\n\nname_words = str.split(' '.join(\n    df['Name'].map(lambda name: ''.join(filter(is_alnum_or_space, name)))\n))\nword_counts = pd.Series(name_words).value_counts().reset_index()\nword_counts.columns = ['Word', 'Count']\n\nax = word_counts.head(10).plot(\n    kind='barh',\n    x='Word',\n    y='Count',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nNote:\nIt doesn’t seem like this extracts much information, since common names like “William”, “John”, and “Henry” are captured as common words in names. But, we may be able to extract the person’s title from the structure of the name: nearly every name is structured as “LastName, Title. FirstName (Alt Name)”\n\n\n# What are the most common titles?\ndef extract_name_title(name: str) -&gt; str:\n    '''\n    Get a person's title out of their name.\n    '''\n    title_and_firstname = name.split(', ')[1]\n    title = title_and_firstname.split('. ')[0]\n    return title\n\n\nname_titles = df['Name'].map(extract_name_title).value_counts().to_frame()\nname_titles = name_titles.reset_index()\nname_titles.columns = ['Title', 'Count']\nname_titles = name_titles.sort_values(by='Count', ascending=True)\nax = name_titles.plot(\n    kind='barh',\n    x='Title',\n    y='Count',\n    xlim=(0, 600),\n    title='Titles Within Passenger Names',\n    legend=False,\n    xlabel='Count',\n    ylabel='Title',\n)\nadd_bar_labels(ax, fmt='{:.0f}', label_type='edge', color='black')\nplt.show()\n\n\n\n\n\n\n\n\n\n# How strong is the correlation between variables?\nax = sns.heatmap(df.corr(numeric_only=True), cmap='RdYlGn', annot=True)\nax.set_title('Feature Correlation')\nplt.show()\n\n\n\n\n\n\n\n\n\n# What were the most common cabins?\ndf \\\n    .loc[~df.Cabin.isna(), 'Cabin'] \\\n    .map(lambda s: s[0]) \\\n    .value_counts() \\\n    .sort_index(ascending=False) \\\n    .reset_index() \\\n    .plot(\n        kind='barh',\n        x='Cabin',\n        y='count',\n        xlim=(0, 70),\n        title='Distribution of Passengers Across Decks',\n        legend=False,\n        xlabel='Count',\n        ylabel='Cabin',\n    )\nadd_bar_labels(ax, fmt='{:.0f}', label_type='edge', color='black')\n\nplt.show()"
  },
  {
    "objectID": "projects/titanic.html#data-preparation",
    "href": "projects/titanic.html#data-preparation",
    "title": "Titanic",
    "section": "3. Data Preparation",
    "text": "3. Data Preparation\nData scientists love to remind each other that roughly 80% of the project involves data preparation. It’s a good rule of thumb, but ultimately is meant to remind us that we should take extra time to synthesize our data.\nSome steps we will take:\n\nClean Data\nTransform Data\n\nTypically, the Transform step is done in 4 parts: Select, Construct, Integrate, and Format. For our purposes, we won’t need to integrate with any additional sources, other than some minor details for the Embarked field. Data formatting is covered pretty thoroughly in the Cleaning stage, so the main step left is the selection of useful features and construction by means of transformation/encoding.\n\nClean Data\n\nPlan\nMost of our data is pretty clean for what we need. There are just some null values that need filling:\n\n\n\nColumn\nStrategy\n\n\n\n\nAge\nMedian-Value Fill\n\n\nCabin\nDefault-Value Fill\n\n\nEmbarked\nBackground Research (2 values)\n\n\n\n\ndf_clean = df.copy()   # Preserve raw data\n\n\n\nAge: Median-Value Fill\nTo balance simplicity of analysis with as beneficial of an imputation as possible, we can look to use the median age by Sex and Pclass to fill the null values of Age.\n\ndef median_value_fill(\n        df: pd.DataFrame,\n        column: str,\n        group_by: str | list[str],\n) -&gt; pd.DataFrame:\n    median_column = f'Median{column}'\n    median_values = df \\\n        .groupby(group_by)[column].median() \\\n        .reset_index() \\\n        .rename(columns={column: median_column})\n    df = df.merge(median_values)\n    df[column] = df[column].fillna(df[median_column])\n    df = df.drop(median_column, axis=1)\n    return df\n\n\ndf_clean = df_clean.pipe(median_value_fill, 'Age', ['Sex', 'Pclass'])\n\n\n\nCabin: Default-Value Fill\n\ndf_clean['Cabin'] = df_clean.Cabin.fillna('Unknown')\n\n\n\nEmbarked: Background Research\n\ndf.loc[df.Embarked.isna()]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n61\n62\n1\n1\nIcard, Miss. Amelie\nfemale\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n829\n830\n1\n1\nStone, Mrs. George Nelson (Martha Evelyn)\nfemale\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n\n\n\n\n\nThere are only 2 passengers who are missing the Embarked field, both with first-class tickets:\n\nMiss (Rose) Amelie Icard, age 38.\nMrs. Martha Evelyn Stone, age 62.\n\nSince there are only 2 passengers without this field, it makes sense to do a little extra research to find where they embarked from. There is an excellent site called Encyclopedia Titanica that has information on many passengers. It helps that both of these passengers survived to tell their story.\nFrom some quick research, we can see that both Amelie and Martha boarded from Southampton, so we can fill their missing values with ‘S’.\nLastly, to help with the interpretation of feature importance, we can change the Embarked field to the full name. This will make it easier to read later, since ‘Southampton’ is faster to interpret than just ‘S’.\n\n\ndf_clean['Embarked'] = df_clean.Embarked.fillna('S')\n\nAside: A Reminder of the Human Stories Behind the Data\nThese 2 passengers were actually in the same group: Amelie boarded the Titanic as the maid to Martha Evelyn Stone. Encyclopedia Titanica gives a brief story of their experience when the Titanic struck the iceberg:\n\nMrs Stone boarded the Titanic in Southampton on 10 April 1912 and was traveling in first class with her maid Amelie Icard. She occupied cabin B-28.\n\n\nMartha was awake in bed when the Titanic struck the iceberg. She slipped a kimono over her night dress, put on her slippers, and went out into the corridor and found other people similarly attired. She asked a crew member if they had struck an iceberg. “Yes,” he said, “but there is no danger. Go back to bed and to sleep.” At this time, Mrs Stone could hear the roar of the steam blowing off and she asked the officer why they were doing this. He told her they had stopped to see what damage there was and that there wasn’t any danger.\n\n\nShe went back to bed and never received a warning. The roaring steam went on for what seemed like forever so she got up and dressed and stepped out into the corridor. There, the daughter of the woman across the hall came running down the corridor, telling her to put on her life preserver and that they must get into the boats. Stone hurried to deck with the woman. They found the sailors getting into the lifeboats, but that there was no real order in loading the boats.\n\n\nStone and her maid got into lifeboat 6 and were rescued. She thought there were about 20 women and two men in the boat. Her role in the boat was to stand on the plug, which she did for seven hours. Another woman waved the only lantern they had in the boat for seven hours. Mrs Stone was sharply critical of how the Titanic crew handled the dilemma they faced that night.\n\n\n\n&lt;img\n  src=\"https://www.encyclopedia-titanica.org/files/2013/201302/admin/images/amelia-icard-titanic-1951.jpg\"\n  width=\"400\"\n  height=\"300\"\n/&gt;\n&lt;div style=\"text-align: center;\"&gt;Amelie Icard, cerca 1950&lt;/div&gt;\n\n\n&lt;img\n  src=\"https://www.encyclopedia-titanica.org/images/martha-stone.jpg\"\n  width=\"200\"\n  height=\"300\"\n/&gt;\n&lt;div style=\"text-align: center;\"&gt;Martha Stone, cerca 1912&lt;/div&gt;\n\n\n\n\n\nTransform Data\n\nPlan\n\n\n\n\n\n\n\nColumn\nTransformations\n\n\n\n\nPassengerId\nSet as Index\n\n\nSurvived\nNone\n\n\nPclass\nOne-Hot Encoding\n\n\nName\nExtract Title\n\n\nSex\nOne-Hot Encoding\n\n\nAge\nOrdinal Encoding -&gt; One-Hot Encoding\n\n\nSibSp, Parch\nExtract FamilySize -&gt; Ordinal Encoding -&gt; One-Hot Encoding\n\n\nTicket\nDrop\n\n\nFare\nOrdinal Encoding -&gt; One-Hot Encoding\n\n\nCabin\nExtract Deck\n\n\nEmbarked\nOne-Hot Encoding\n\n\n\n\ndf_transformed = df_clean.copy()\n\n\n\nPassengerId: Set as Index\n\nif 'PassengerId' in df_transformed.columns:\n    df_transformed = df_transformed.set_index('PassengerId')\n\n\n\nPclass: One-Hot Encoding\n\nif 'Pclass' in df_transformed.columns:\n    pclass_map = {1: 'FirstClass', 2: 'SecondClass', 3: 'ThirdClass'}\n    df_transformed['Pclass'] = df_transformed.Pclass.map(pclass_map)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='',\n        prefix_sep='',\n        columns=['Pclass'],\n        dtype=int\n    )\n\n\n\nName: Extract Title\n\ndef get_binned_title_from_name(name: str) -&gt; str:\n    title_and_firstname = name.split(', ')[1]\n    title = title_and_firstname.split('. ')[0]\n    if title in ['Mr', 'Master']:\n        return 'MrMaster'\n    elif title in ['Miss', 'Mrs', 'Ms']:\n        return 'MissMrsMs'\n    else:\n        return 'Other'\n\n\nif 'Name' in df_transformed.columns:\n    df_transformed['Title'] = df_transformed['Name'] \\\n        .map(get_binned_title_from_name)\n    df_transformed = df_transformed.drop('Name', axis=1)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='Title',\n        prefix_sep='',\n        columns=['Title'],\n        dtype=int\n    )\n\n\n\nSex: One-Hot Encoding\n\nif 'Sex' in df_transformed.columns:\n    df_transformed['Male'] = df_transformed['Sex'].eq('male').astype(int)\n    df_transformed.drop('Sex', axis=1, inplace=True)\n\n\n\nAge: Ordinal Encoding -&gt; One-Hot Encoding\n\ndef apply_ordinal_to_one_hot(\n        df: pd.DataFrame,\n        column: str,\n        q: int\n) -&gt; pd.DataFrame:\n    df = df.copy()\n    categories = pd.qcut(df[column], q).cat.categories\n    labels = [\n        '{:.0f}To{:.0f}'.format(cat.left, cat.right)\n        for cat in categories\n    ]\n    df[column] = pd.qcut(df[column], q, labels)\n    return pd.get_dummies(df, columns=[column], prefix_sep='', dtype=int)\n\n\nif 'Age' in df_transformed.columns:\n    df_transformed = df_transformed.pipe(apply_ordinal_to_one_hot, 'Age', q=6)\n\n\n\nSibSp / Parch: Extract FamilySize -&gt; Ordinal Encoding -&gt; One-Hot Encoding\nIt’s likely that, while SibSp and Parch may not be useful alone, the combination gives us insight: the size of the family/group. We can also apply binning here to group passenger family sizes, whether they traveled alone (which was fairly common) or had a small, medium, or large family.\n\ndef encode_family_size(size: int) -&gt; str:\n    if size == 1:\n        return 'Alone'\n    elif size &lt;= 4:\n        return 'SmallFamily'\n    elif size &lt;= 6:\n        return 'MediumFamily'\n    else:\n        return 'LargeFamily'\n\n\nif 'SibSp' in df_transformed.columns:\n    cols = ['SibSp', 'Parch']\n    df_transformed['FamilySize'] = df_transformed[cols] \\\n        .sum(axis=1) \\\n        .add(1) \\\n        .map(encode_family_size)\n    df_transformed.drop(cols, axis=1, inplace=True)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='',\n        prefix_sep='',\n        columns=['FamilySize'],\n        dtype=int\n    )\n\n\n\nTicket: Drop\n\nif 'Ticket' in df_transformed.columns:\n    df_transformed.drop('Ticket', axis=1, inplace=True)\n\n\n\nFare: Ordinal Encoding -&gt; One-Hot Encoding\n\nif 'Fare' in df_transformed.columns:\n    df_transformed = df_transformed.pipe(apply_ordinal_to_one_hot, 'Fare', q=6)\n\n\ndf_transformed\n\n\n\n\n\n\n\n\nSurvived\nCabin\nEmbarked\nFirstClass\nSecondClass\nThirdClass\nTitleMissMrsMs\nTitleMrMaster\nTitleOther\nMale\n...\nAlone\nLargeFamily\nMediumFamily\nSmallFamily\nFare-0To8\nFare8To9\nFare9To14\nFare14To26\nFare26To52\nFare52To512\n\n\nPassengerId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\nUnknown\nS\n0\n0\n1\n0\n1\n0\n1\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n2\n1\nC85\nC\n1\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n1\nUnknown\nS\n0\n0\n1\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n1\nC123\nS\n1\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n0\nUnknown\nS\n0\n0\n1\n0\n1\n0\n1\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n887\n0\nUnknown\nS\n0\n1\n0\n0\n0\n1\n1\n...\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n888\n1\nB42\nS\n1\n0\n0\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n889\n0\nUnknown\nS\n0\n0\n1\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n890\n1\nC148\nC\n1\n0\n0\n0\n1\n0\n1\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n891\n0\nUnknown\nQ\n0\n0\n1\n0\n1\n0\n1\n...\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n891 rows × 26 columns\n\n\n\n\n\nCabin: Extract Deck\nWhile much of the cabin numbers are null, we can still extract some info from the available numbers. From some background research, we know a few things about the Titanic decks:\n\nDecks A, B, and C only contained first-class passengers.\nDecks D and E had any class passenger.\nDecks F and G only held second- and third-class passengers.\n\nThere is an odd deck: T. This passenger is first-class, so we can bucket him into the ‘A’ deck with other first-class passengers.\nWe can group these decks as ‘ABC’, ‘DE’, and ‘FG’, leaving ‘X’ for any unknown decks.\n\ndef get_deck_from_cabin(cabin) -&gt; str:\n    default_deck = 'X'\n\n    if pd.isna(cabin):\n        return default_deck\n    \n    deck = str(cabin)[0]\n    for bucket in ['ABC', 'DE', 'FG']:\n        if deck in bucket:\n            return bucket\n    else:\n        return default_deck\n\n\nif 'Cabin' in df_transformed.columns:\n    df_transformed['Deck'] = df_transformed.Cabin.map(get_deck_from_cabin)\n    df_transformed.drop('Cabin', axis=1, inplace=True)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='Deck',\n        prefix_sep='',\n        columns=['Deck'],\n        dtype=int\n    )\n\n\n\nEmbarked: One-Hot Encoding\n\nif 'Embarked' in df_transformed.columns:\n    cities_map = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n    df_transformed['Embarked'] = df_transformed.Embarked.map(cities_map)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='Embarked',\n        prefix_sep='',\n        columns=['Embarked'],\n        dtype=int\n    )\n\n\ndf_transformed.head()\n\n\n\n\n\n\n\n\nSurvived\nFirstClass\nSecondClass\nThirdClass\nTitleMissMrsMs\nTitleMrMaster\nTitleOther\nMale\nAge0To19\nAge19To24\n...\nFare14To26\nFare26To52\nFare52To512\nDeckABC\nDeckDE\nDeckFG\nDeckX\nEmbarkedCherbourg\nEmbarkedQueenstown\nEmbarkedSouthampton\n\n\nPassengerId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n2\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n3\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n0\n0\n0\n1\n0\n1\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n5 rows × 31 columns\n\n\n\nLook how clean our data looks!"
  },
  {
    "objectID": "projects/titanic.html#modeling",
    "href": "projects/titanic.html#modeling",
    "title": "Titanic",
    "section": "4. Modeling",
    "text": "4. Modeling\nWe finally made it to the fun part, the modeling! CRISP-DM suggests to iterate model building and assessment until you strongly believe that you have found the best model. This can be accomplished in 4 stages:\n\nSelect modeling techniques\nGenerate test design\nBuild model\nAssess model\n\nThe plan is to use steps 1-2 to select several different models and setup the experiments, then apply steps 3-4 to each of the models to train/evaluate the performance of each.\n\nSelect modeling techniques\nI hinted at this earlier, but there are several models we can apply here:\n\nLogistic Regression\nNaive-Bayes\nk-Nearest Neighbors\nDecision Tree\nRandom Forest\nGradient-Boosted Trees\nAda Boost\nSupport Vector Machines\n\nWhile we could go as far as looking at neural networks to introduce more sophisticated feature interactions, this may be overkill. However, if we don’t get the performance we’d like out of these models, we can circle back to deep learning methods.\n\n\nGenerate test design\nAs typical with machine learning, we’ll need to split our data into training/ testing data. From there, we can apply cross-validation to each model to determine the most optimal parameters for the model. A summary of our approach can be seen below.\n\n\n\nFor measuring performance, we could look at an F1 score to account for precision/recall. However, since the overall survival rate isn’t significantly different than the non-survival rate, a simple accuracy measure should suffice here.\nFinally, to tune the hyperparameters, we can use a grid search to find the optimal parameters. Note that, as the diagram above shows, the hyperparameter tuning will be done with a validation set, leaving the test set for the final model evaluation.\nWe can store the model, grid search results, hyperparameters, and performance scores in a DataFrame for easy access.\n\n# Split train/test datasets\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\ntarget = 'Survived'\ny = df_transformed[target].copy()\nX = df_transformed.drop(target, axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n\n\nBuild model\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Setup dataframe to store model information\nmodel_names = [\n    'AdaBoost',\n    'DecisionTree',\n    'NaiveBayes',\n    'GradientBoosted',\n    'KNN',\n    'LogisticRegression',\n    'RandomForest',\n    'SVC',\n]\ncolumns = ['Model', 'GridSearchResults', 'OptimalParams', 'Accuracy']\nmodels = pd.DataFrame(index=model_names, columns=columns)\nmodels\n\n\n\n\n\n\n\n\nModel\nGridSearchResults\nOptimalParams\nAccuracy\n\n\n\n\nAdaBoost\nNaN\nNaN\nNaN\nNaN\n\n\nDecisionTree\nNaN\nNaN\nNaN\nNaN\n\n\nNaiveBayes\nNaN\nNaN\nNaN\nNaN\n\n\nGradientBoosted\nNaN\nNaN\nNaN\nNaN\n\n\nKNN\nNaN\nNaN\nNaN\nNaN\n\n\nLogisticRegression\nNaN\nNaN\nNaN\nNaN\n\n\nRandomForest\nNaN\nNaN\nNaN\nNaN\n\n\nSVC\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef apply_grid_search(estimator: BaseEstimator, params: dict) -&gt; pd.Series:\n    model = GridSearchCV(estimator, params, scoring='accuracy', n_jobs=-1)\n    model.fit(X_train, y_train)\n    accuracy = model.score(X_test, y_test)\n    return pd.Series(dict(\n        Model=model.best_estimator_,\n        GridSearchResults=pd.DataFrame(model.cv_results_),\n        OptimalParams=model.best_params_,\n        Accuracy=accuracy,\n    ))\n\n\nNaive-Bayes\n\nparams = {'var_smoothing': np.logspace(-12, -7, 6)}\nmodels.loc['NaiveBayes'] = apply_grid_search(GaussianNB(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['NaiveBayes', 'Accuracy']))\nmodels.loc['NaiveBayes', 'Model']\n\nAccuracy: 78.21%\n\n\nGaussianNB(var_smoothing=np.float64(1e-12))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNB?Documentation for GaussianNBiFitted\n        \n            \n                Parameters\n                \n\n\n\n\npriors \nNone\n\n\n\nvar_smoothing \nnp.float64(1e-12)\n\n\n\n\n            \n        \n    \n\n\n\n\nLogistic Regression\n\nparams = {\n    'penalty':      ['l2', 'l1'],\n    'C':            np.logspace(-3, 3, 7),\n    'solver':       ['liblinear', 'saga'],\n    'class_weight': [None, 'balanced'],\n}\nmodels.loc['LogisticRegression'] = apply_grid_search(LogisticRegression(),\n                                                     params)\nprint('Accuracy: {:.2%}'.format(models.loc['LogisticRegression', 'Accuracy']))\nmodels.loc['LogisticRegression', 'Model']\n\nAccuracy: 83.24%\n\n\nLogisticRegression(C=np.float64(1.0), penalty='l1', solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l1'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \nnp.float64(1.0)\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'liblinear'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nK-Nearest Neighbors\n\nparams = {\n    \"n_neighbors\": [3, 5, 7, 11, 15],\n    \"weights\":     [\"uniform\", \"distance\"],\n    \"p\":           [1, 2],\n}\nmodels.loc['KNN'] = apply_grid_search(KNeighborsClassifier(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['KNN', 'Accuracy']))\nmodels.loc['KNN', 'Model']\n\nAccuracy: 77.09%\n\n\nKNeighborsClassifier(n_neighbors=15, p=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n15\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n1\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nDecisionTree\n\nparams = {\n    \"criterion\":         [\"gini\", \"entropy\", \"log_loss\"],\n    \"max_depth\":         [None, 5, 10, 20],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\":  [1, 2, 5],\n    \"class_weight\":      [None, \"balanced\"],\n}\nmodels.loc['DecisionTree'] = apply_grid_search(DecisionTreeClassifier(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['DecisionTree', 'Accuracy']))\nmodels.loc['DecisionTree', 'Model']\n\nAccuracy: 82.68%\n\n\nDecisionTreeClassifier(min_samples_leaf=5, min_samples_split=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n5\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nSupport Vector Classifier (SVC)\n\nparams = {\n    \"kernel\":       [\"linear\", \"rbf\"],\n    \"C\":            np.logspace(-2, 2, 5),\n    \"gamma\":        [\"scale\", \"auto\"],\n    \"class_weight\": [None, \"balanced\"],\n}\nmodels.loc['SVC'] = apply_grid_search(SVC(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['SVC', 'Accuracy']))\nmodels.loc['SVC', 'Model']\n\nAccuracy: 80.45%\n\n\nSVC(C=np.float64(1.0))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC \nnp.float64(1.0)\n\n\n\nkernel \n'rbf'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nRandomForest\n\nparams = {\n    \"n_estimators\":     [200, 400],\n    \"max_depth\":        [None, 10, 20],\n    \"max_features\":     [\"sqrt\", \"log2\"],\n    \"min_samples_leaf\": [1, 2, 5],\n    \"class_weight\":     [None, \"balanced\"],\n}\nmodels.loc['RandomForest'] = apply_grid_search(RandomForestClassifier(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['RandomForest', 'Accuracy']))\nmodels.loc['RandomForest', 'Model']\n\nAccuracy: 83.24%\n\n\nRandomForestClassifier(max_depth=10, min_samples_leaf=2, n_estimators=400)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n400\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n10\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n2\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nAda-Boosted Decision Tree\n\nparams = {\n    \"n_estimators\":  [100, 300, 600],\n    \"learning_rate\": [0.01, 0.1, 0.5, 1.0],\n    'algorithm':     ['SAMME']\n}\nestimator = AdaBoostClassifier(models.loc['DecisionTree', 'Model'])\nmodels.loc['AdaBoost'] = apply_grid_search(estimator, params)\nprint('Accuracy: {:.2%}'.format(models.loc['AdaBoost', 'Accuracy']))\nmodels.loc['AdaBoost', 'Model']\n\nc:\\programming\\repos\\website\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n  warnings.warn(\n\n\nAccuracy: 83.80%\n\n\nAdaBoostClassifier(algorithm='SAMME',\n                   estimator=DecisionTreeClassifier(min_samples_leaf=5,\n                                                    min_samples_split=5),\n                   learning_rate=0.01, n_estimators=100)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifier?Documentation for AdaBoostClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nDecisionTreeC...mples_split=5)\n\n\n\nn_estimators \n100\n\n\n\nlearning_rate \n0.01\n\n\n\nalgorithm \n'SAMME'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    estimator: DecisionTreeClassifierDecisionTreeClassifier(min_samples_leaf=5, min_samples_split=5)DecisionTreeClassifier?Documentation for DecisionTreeClassifier\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n5\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nGradient-Boosted Tree\n\nparams = {\n    \"n_estimators\":  [100, 300],\n    \"learning_rate\": [0.01, 0.05, 0.1],\n    \"max_depth\":     [2, 3, 5],\n    \"subsample\":     [1.0, 0.8],\n    \"max_features\":  [None, \"sqrt\", \"log2\"],\n}\nestimator = GradientBoostingClassifier()\nmodels.loc['GradientBoosted'] = apply_grid_search(estimator, params)\nprint('Accuracy: {:.2%}'.format(models.loc['GradientBoosted', 'Accuracy']))\nmodels.loc['GradientBoosted', 'Model']\n\nAccuracy: 81.56%\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=5, max_features='sqrt',\n                           n_estimators=300)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'log_loss'\n\n\n\nlearning_rate \n0.01\n\n\n\nn_estimators \n300\n\n\n\nsubsample \n1.0\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n5\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_features \n'sqrt'\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n\n\n\n\nAssess model\n\nax = models['Accuracy'].sort_values().plot(kind='barh', title='Model Accuracy')\nadd_bar_labels(ax, '{:.2%}')\nplt.show()"
  },
  {
    "objectID": "projects/titanic.html#evaluation",
    "href": "projects/titanic.html#evaluation",
    "title": "Titanic",
    "section": "5. Evaluation",
    "text": "5. Evaluation\nThrough the Titanic data science workflow, several key insights emerged:\n\nSurvival Rates: First-class passengers and females had significantly higher survival rates. Fare and age also showed notable correlations with survival.\nFeature Engineering: Extracting titles from names and grouping family sizes improved model interpretability and performance.\nData Cleaning: Most columns were clean, with targeted imputation for missing ages and embarked locations. Cabin data was largely missing but deck extraction provided some value.\nModeling: Multiple models were evaluated using grid search and cross-validation. Ensemble methods (Random Forest, Gradient Boosted Trees, AdaBoost) generally outperformed simpler models.\nBest Models: The highest accuracy was achieved by ensemble tree-based models, confirming the value of engineered features and robust preprocessing.\n\nOverall, the workflow demonstrated the importance of thorough data understanding, careful cleaning, and feature engineering in building effective predictive models for Titanic survival.\n\nmodel_coefficients = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': models.loc['LogisticRegression', 'Model'].coef_[0]\n})\nmodel_coefficients.set_index('Feature', inplace=True)\n\nplt.figure(figsize=(3, 8))\nsns.heatmap(model_coefficients, cmap='RdYlGn')\nplt.title('Logistic Regression Coefficients')\nplt.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "website",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  }
]