[
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Titanic\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Posts",
    "section": "",
    "text": "Stochastic Gradient Descent\n\n\nThe Effects of Batch Size on Convergence\n\n\n\n\n\n\n\n\nSep 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nWikidata: A Collaborative, Multilingual Knowledge Graph\n\n\n\n\n\n\n\n\n\n\n\nJul 31, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nData Lifecycle 101\n\n\nHow to Stay Relevant Beyond a Single Project\n\n\n\n\n\n\n\n\nFeb 24, 2025\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Matthew Burns",
    "section": "",
    "text": "Welcome to my Quarto website! I decided to start this as a way to showcase my data science projects, but found it a great way to share insights, tips, and other helpful information related to data science."
  },
  {
    "objectID": "index.html#projects",
    "href": "index.html#projects",
    "title": "Matthew Burns",
    "section": "Projects",
    "text": "Projects\n\nTitanic"
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html",
    "href": "posts/data-lifecycle-101/index.html",
    "title": "Data Lifecycle 101",
    "section": "",
    "text": "Ever wonder how datasets used for one project end up fueling countless others? From Netflix recommendations to COVID-19 dashboards, data lives far beyond its original intended use, all thanks to the data lifecycle.\nThe data lifecycle is the end-to-end process that ensures that data remains useful, accurate, and accessible long after its first use. Without explicit and careful curation throughout the lifecycle, datasets can become outdated, unreliable, or even lost.\nIn this post, I will break down the key stages of the data lifecycle and why they matter for long-term value."
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html#what-is-the-data-lifecycle",
    "href": "posts/data-lifecycle-101/index.html#what-is-the-data-lifecycle",
    "title": "Data Lifecycle 101",
    "section": "What Is the Data Lifecycle?",
    "text": "What Is the Data Lifecycle?\nThe data lifecycle refers to the complete journey data takes—from collection and storage to analysis and sharing, and eventually disposal of the data. It ensures that data can be repurposed across a variety of projects, industries, and even decades.\nUnlike project-based work or problem-solving, which is time-bound, data often outlives the project for which it was collected. This longevity makes proper lifecycle management essential to avoid duplication, ensure data quality, and enable future discoveries from the data."
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html#the-stages-of-the-data-lifecycle",
    "href": "posts/data-lifecycle-101/index.html#the-stages-of-the-data-lifecycle",
    "title": "Data Lifecycle 101",
    "section": "The Stages of the Data Lifecycle",
    "text": "The Stages of the Data Lifecycle\nThe data lifecycle consists of the following key stages:\n\nPlanning and collection\nIngestion and Storage\nDescription and Metadata\nProcessing and Quality Assurance\nAnalysis and Utilization\nPreservation and Curation\nPublication and Sharing\nArchival and Disposal\n\nEach stage of the data lifecycle has its own characteristics that add value to the life of the data. Let’s dig into each step in more detail:\n\n1. Planning and Collection\nThe lifecycle begins with planning and collection, where organizations decide what data to gather and how to collect it. This stage requires careful consideration, especially when collecting data in complex environments.\nFor instance, installing sensors on remote mountain tops to measure snowpack demand thoughtful planning, such as overseeing the power supply to the sensors and transmitting data from hard-to-reach areas. Without careful preparation, data collection efforts can be inefficient, incomplete, or unusable.\n\n\n2. Ingestion and Storage\nOnce the data is collected, it moves on to the ingestion & storage phase. This involves capturing the data and organizing it in data storage systems, like databases or data lakes, to make sure it can be easily retrieved for use in the future.\nRolls-Royce is an excellent example in this phase. They collect telemetry data from their aircraft engines during flight, storing it at regular intervals. They will use this to monitor engine performance and predict the maintenance requirements of the aircraft.\n\n\n3. Description and Metadata\nTo make datasets understandable and reusable, they must be documented with metadata—effectively, data about the data. Metadata describes not just the dataset’s structure, but its context, origin, and any transformation it has undergone.\nThe National Archives, for example, meticulously tags its electronic records with metadata to ensure that users can clearly interpret and work with the records effectively in the future. Standardized schemas and ontologies make it easier to share and merge data across disciplines.\n\n\n4. Processing and Quality Assurance\nRaw data is rarely useful in its original state; it must be cleaned and transformed before it is ready to be analyzed. This stage, known as processing and quality assurance, ensures consistency, accuracy, and reliability prior to the analysis.\nAutomation plays a crucial role here, reducing human error and streamline the process. A web scraping project, for example, might involve filtering out duplicate entries, standardizing date formats, and flagging anomalies before the data is ready for the analysis.\n\n\n5. Analysis and Utilization\nOnce the data has been cleaned and its quality has been verified, the data can then be analyzed and utilized to extract insights and drive decision-making. This is the phase where predictive models, dashboards, and reports come into play.\nNetflix exemplifies this stage well with their recommendation systems. They will use their data to analyze user behavior to recommend shows based on the user’s viewing history and preferences. This personalizes each user’s experience, helping to maintain engagement and success within their platform.\n\n\n6. Preservation and Curation\nPreservation and curation ensure the data remains accessible and usable over time. Following FAIR data principles (Findable, Accessible, Interoperable, and Reusable) helps maintain data integrity and discoverability.\nIn astronomy, researchers often reanalyze old photographic plates to discover previously undetected supernovae, demonstrating how well-preserved data can fuel new discoveries, even decades after collection.\n\n\n7. Publication and Sharing\nWhen datasets are ready to be used in a broader context, they enter the publication and sharing stage. This involves making datasets available with clear usage criteria, documentation, and licensing.\nPublic health organizations, for example, published COVID-19 datasets worldwide, allowing researchers to work with the data in real-time and advance the pandemic response efforts.\n\n\n8. Archival/Disposal\nEventually, datasets will become outdated or irrelevant, bringing them to the archival/disposal phase. These kinds of datasets must be securely archived or disposed of by following all regulatory guidelines.\nAn example of this is legal records, which are often retained for a specific period before they are securely deleted. This explicit process of handling old data helps maintain compliance and avoid unnecessary storage costs."
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html#why-it-matters-the-value-of-lifecycle-management",
    "href": "posts/data-lifecycle-101/index.html#why-it-matters-the-value-of-lifecycle-management",
    "title": "Data Lifecycle 101",
    "section": "Why It Matters: The Value of Lifecycle Management",
    "text": "Why It Matters: The Value of Lifecycle Management\nEffective data lifecycle management helps ensure that good data never truly dies. It evolves, supports new insights, and drives innovation across projects and disciplines. Good management of this lifecycle promotes data integrity, redundancy, and reusability.\nIn scientific research, data collected decades ago can continue to fuel new discoveries, from climate change studies to advancements in medicine. Without proper lifecycle practices, we risk valuable datasets becoming obsolete, underutilized, or lost in the deluge of data."
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html#challenges-and-best-practices",
    "href": "posts/data-lifecycle-101/index.html#challenges-and-best-practices",
    "title": "Data Lifecycle 101",
    "section": "Challenges and Best Practices",
    "text": "Challenges and Best Practices\nManaging data throughout the lifecycle is not always easy. For instance, technology can become obsolete, rendering datasets unreadable as platforms and formats evolve. Privacy regulations, such as HIPAA, add to the complexity of data storage and sharing. Further, the sheer volume of data can drive up storage costs, making effective infrastructure an essential practice.\nWe must keep best practices when we manage our data’s lifecycle. This means automating our data pipelines to ensure consistency and reliability while transforming our data. It means using standard formats like JSON or XML for better interoperability and maintaining clear documentation for each dataset. Regular audits, quality checks, and clear metadata updates will further enhance our data’s usability and longevity."
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html#conclusion",
    "href": "posts/data-lifecycle-101/index.html#conclusion",
    "title": "Data Lifecycle 101",
    "section": "Conclusion",
    "text": "Conclusion\nThe data lifecycle transforms raw data into a long-term asset, ensuring that it remains valuable, accessible, and trustworthy. From collection to archival, every stage has its own importance. We must think not just about our current projects, but about all future innovations.\nBy taking care to manage data, adopt best practices, and ensure quality throughout its lifecycle, we can turn our data into a persisting resource that moves beyond a one-time tool."
  },
  {
    "objectID": "posts/data-lifecycle-101/index.html#references",
    "href": "posts/data-lifecycle-101/index.html#references",
    "title": "Data Lifecycle 101",
    "section": "References",
    "text": "References\n\nNational Archives. Managing Electronic Records. (Link)\nWilkinson, M. D. et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data, 3, 160018. (Link)\nMarr, B. (2015). How Big Data Drives Success At Rolls-Royce. Forbes. (Link)"
  },
  {
    "objectID": "posts/wikidata-overview/index.html",
    "href": "posts/wikidata-overview/index.html",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "",
    "text": "“Wikidata is a free, collaborative, multilingual, secondary knowledge base, collecting structured data to provide support for Wikipedia, Wikimedia Commons, the other wikis of the Wikimedia movement, and to anyone in the world.” – Wikidata’s Self-Description\nLet’s take a step back to the year 2012. When searching for information, we often consulted with the collaborative realm of Wikipedia, despite cautions from our English teachers. This massive source provided a wealth of information, ranging from ancient historical facts to governmental structures to Pokémon types.\nDespite its usefulness, Wikipedia had 2 main problems: First, it held wide variation in content between languages. We take this for granted as English-speakers, but when providing information to billions of people speaking thousands of languages, making that many updates simply isn’t feasible. Second, while Wikipedia is useful for one-off research needs, it provides little benefit when trying to aggregate vast amounts of information. Imagine creating a Python webscraper for all of Wikipedia, parsing the content from the HTML documents, utilizing natural language processing to extract knowledge from the raw information… You get the picture. Not exactly the easiest approach.\nEnter Wikidata. Created by the Wikimedia Deutschland association in October 2012, the project sought to build on Wikipedia by providing consistent, highly findable data across a variety of domains. The concept of the semantic web is prevalent in Wikidata, showcasing the creators’ emphasis on meaningful, interrelated links between pages. In addition, Wikidata allows its data to be edited by anyone."
  },
  {
    "objectID": "posts/wikidata-overview/index.html#free-structured-and-collaborative",
    "href": "posts/wikidata-overview/index.html#free-structured-and-collaborative",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "Free, Structured, and Collaborative",
    "text": "Free, Structured, and Collaborative\nWikidata acts as a database for a wide variety of data, much of which is sourced from other Wikimedia sites like Wikipedia, Wikibooks, or Wiktionary. The knowledge base can be accessed and modified by anyone. This provides 3 main benefits:\n\nFree & Accessible: Like Wikipedia, Wikidata is published under the Creative Commons license, letting you copy, modify, or distribute the data for any purpose, without requiring permission, and from anywhere on the planet.\nStructured: The site provides multiple mechanisms to access its content. While the most obvious interface is through their website, wikidata.org, it is also possible to query its data using SPARQL queries at query.wikidata.org. This bot-friendly approach means Wikidata’s content and be aggregated for all sorts of applications.\nCollaborative: The content itself is maintained by Wikidata editors, meaning you don’t have to manage the data’s structure and accuracy yourself. The site’s many editors use preset rules for content creation, ensuring consistency and accuracy across its many languages."
  },
  {
    "objectID": "posts/wikidata-overview/index.html#a-repository-of-items",
    "href": "posts/wikidata-overview/index.html#a-repository-of-items",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "A Repository of Items",
    "text": "A Repository of Items\nWikidata is composed of items, each representing a single entity like a person, company, location, language, etc. To prevent duplication, a unique item identifier is given to each item, prefixed with a Q (e.g., Douglas Adams has ID Q42). The more friendly name for an item is called its label, which is typically the name or title we associate with the object. The item’s header also contains a description and aliases to allow for clearer understanding and better findability.\nThe real power within Wikidata comes from its statements: the key-value pairs that provide the context for items. Each statement contains a property along with one or more values that describe it. For example, you may find a person’s sex/gender, a location’s native language, or a company’s logo and founders. If a property’s value links to an external database, it is called an identifier.\n\n\n\nWikidata’s Data Model\n\n\nIn most cases, statements provide a connection to other Wikidata pages, creating a semantic link between entities. While it initially may appear unimportant, this subtle addition creates the mechanism that connects items together, resulting in a complex, interconnected network of information. But how does this help us? Are links really that important?"
  },
  {
    "objectID": "posts/wikidata-overview/index.html#nodes-and-edges",
    "href": "posts/wikidata-overview/index.html#nodes-and-edges",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "Nodes and Edges",
    "text": "Nodes and Edges\nThe links created from Wikidata’s item statements provide a powerful structure to traverse its data. Similar to a graph database, the property values provide meaningful relationships between pages in the form of nodes (Wikidata items) and edges (property values). It’s easy to overlook just how useful these links are. Take Figure 3 for example. Many programmers know that Python is a descendent of C, much in the same way that English is a descendent of Latin. However, many aren’t aware of the far-reaching effects of this relationship.\nWhile C influenced both Python and C++ independently, there are other software packages that benefit from this inheritance. For instance: the Python package NumPy, a package used in numerous other libraries, is written in both Python and C. This package inspired Pandas, which builds on the library by providing stronger analytical tools. While developed separately from Python, C++ is the primary language used in Matplotlib, one of the most common visual tools in a data scientist’s toolkit. If that weren’t enough, there is an additional relationship between NumPy, Pandas, and Matplotlib: they each receive funding from the Chan Zuckerberg Initiative, an organization established by Mark Zuckerberg and his wife, Priscilla Chan. While it certainly conveys the strength of the connection between these items, Figure 3 only scratches the surface of the interconnectedness of Python- and C-based software.\n\n\n\nWikidata Property-Value Graph"
  },
  {
    "objectID": "posts/wikidata-overview/index.html#applications",
    "href": "posts/wikidata-overview/index.html#applications",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "Applications",
    "text": "Applications\nWhile this highly connected structure may seem abstract on the surface, it’s already in wide use across the web. From visual art to historical timelines to semantic search, developers and researchers have tapped into the torrent of information that is Wikidata. While numerous projects have been developed on top of Wikidata, a few are detailed in the following sections.\n\nReasonator\nOne of the most well-known tools is Reasonator, a web application to make Wikidata content more readable and engaging. Its creators believed the default Wikidata interface was too dry, providing more database than story. To compensate for this, they implemented a custom JavaScript class to query and render Wikidata content, dynamically pulling data and formatting the results based on an item’s category. It also provides support for SPARQL queries, translating results directly into their API, which is useful for advanced users and custom displays. The result is a semantic front-end that reads cleaner than the standard Wikidata database.\nFigure 3: Wikidata’s property-value relationships reveal the interconnectedness of Python packages.\n\n\nHistropedia\nAnother stand-out application, Histropedia is an interactive timeline builder that pulls events, people, and cultural landmarks from Wikidata. Targeted at researchers and instructors, the application makes it easy to explore and visualize historical relationships over time.\nUsers can choose from a library of premade timelines, such as “Presidents of the United States”, or can create their own timelines from scratch. The software provides flexibility with custom timeline creation, allowing users to search for items on Wikidata or Wikipedia, then drag and drop them into the site’s WYSIWYG editor. Lastly, filters can be applied to various searches to better refine the query by topic, location, or period. By layering a user-friendly frontend on top of a powerful, knowledge-rich backend, Histropedia makes it simple to turn raw facts into a visually engaging narrative.\n\n\nCrotos\nLast on our list, Crotos exploits the power of Wikidata in a more refined sense. Considering the over 400,000 artworks cataloged in Wikidata, more than half of which are in high definition, Crotos aggregates this collection of art to create a virtual path through an endless art museum. The platform connects to Wikidata’s backend to allow users to search for artworks by medium (e.g., paintings, sculptures, prints), as well as artists, creation date, and a variety of other metadata. Crotos’ clean interface allows for discovery of art, which particularly serves museums, educators, or simply the curious individual looking to gain some culture."
  },
  {
    "objectID": "posts/wikidata-overview/index.html#where-to-begin",
    "href": "posts/wikidata-overview/index.html#where-to-begin",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "Where to Begin",
    "text": "Where to Begin\nGiven its connection to the other Wikimedia sites, Wikidata contains just about any information you can think of. You can search for past U.S. presidents, finding their birth date and death date, their personal signature, and their occupations prior to taking office. You can search for cities across the globe, finding their coordinates, year of inception, and who they are named after. You can find artistic works, scientific methods, company information, universities, historical events, and even past elections.\nThe easiest way to get started is by searching a topic from Wikidata’s main page. Alternatively, the site provides a handful of quick-start guides, from a basic introduction to full-fledged tutorials. Wherever you begin, know that you’re taking a first step onto the limitless, interconnected web of knowledge that that is Wikidata."
  },
  {
    "objectID": "posts/wikidata-overview/index.html#sources",
    "href": "posts/wikidata-overview/index.html#sources",
    "title": "Wikidata: A Collaborative, Multilingual Knowledge Graph",
    "section": "Sources",
    "text": "Sources\n[1] Wikipedia Contributors. (2019, October 14). Wikidata. Wikipedia; Wikimedia Foundation. https://en.wikipedia.org/wiki/Wikidata\n[2] Perez, S. (2012, March 30). Wikipedia’s Next Big Thing: Wikidata, A Machine Readable, User-Editable Database Funded By Google, Paul Allen And Others | TechCrunch. TechCrunch. https://techcrunch.com/2012/03/30/wikipedias-next-bigthing-wikidata-a-machine-readable-user-editable-database-funded-by-google-paulallen-and-others/\n[3] Wikidata:Introduction - Wikidata. (n.d.). Www.wikidata.org. https://www.wikidata.org/wiki/Wikidata:Introduction\n[4] Chalabi, M. (2013, April 26). Welcome to Wikidata! Now what? The Guardian. https://www.theguardian.com/news/datablog/2013/apr/26/wikidata-launch"
  },
  {
    "objectID": "projects/titanic/index.html",
    "href": "projects/titanic/index.html",
    "title": "Titanic",
    "section": "",
    "text": "To most, the Titanic may evokes imagery of a grand ship, a newspaper headline, or maybe the iconic picture of Leo & Kate on the bow of the ship. To data scientists, the Titanic often reminds us of our first dataset, a real dataset, requiring vast amounts of cleaning, preprocessing, transformation, and hyperparameter tuning.\nNevertheless, the Titanic dataset is a great place to start with data science projects. It is a great place to start, covering a great variety of topics."
  },
  {
    "objectID": "projects/titanic/index.html#setup",
    "href": "projects/titanic/index.html#setup",
    "title": "Titanic",
    "section": "Setup",
    "text": "Setup\n\nDefine Workflow\nBefore we start, let’s define the scope of our workflow. I’m taking the steps from the framework CRISP-DM (Cross-Industry Standard Process for Data Mining). Data Science PM has a useful overview of this framework on their website.\n\nBusiness Understanding\nData Understanding\nData Preparation\nModeling\nEvaluation\nDeployment"
  },
  {
    "objectID": "projects/titanic/index.html#business-understanding",
    "href": "projects/titanic/index.html#business-understanding",
    "title": "Titanic",
    "section": "1. Business Understanding",
    "text": "1. Business Understanding\n\nBusiness Objectives\nTypically, we’ll want to thoroughly understand the business side of things before embarking on an analysis. This involves a lot of discussion with stakeholders–the ones who feel the most impact from the problem at hand.\nPerhaps we need an understanding of what went wrong on the Titanic in order to prevent future disasters. Or, maybe we want to analyze the most impactful features of passengers in their survival so as to determine the effect of society norms in the early 20th century on a passenger surviving any kind of disaster.\nRegardless of the business case, we’re looking to understand survival of Titanic passengers and create a model to predict which passengers will survive.\n\n\nAssess the Situation\nHere is where we would need to evaluate what resources we have available. If our model was computationally intensive, what resources does our company have with cloud computing? Are there other metadata or supplemental datasets that may be useful here?\nFor our purpose, we simply have the dataset provided to us by Kaggle. As we get further into the analysis, we can evaluate risks/contingencies based on the cleanliness of the data.\n\n\nDetermine the Goals\n\nIn short, what does success look like?\n\nPersonally, I would love to see a model that can predict (with excellent accuracy) which passengers will survive the Titanic. I anticipate that the bare features alone won’t cut it, so a successful model will likely have well-engineered features that make use of the raw data. Finally, a successful project would be one that considers multiple models, using cross-validation, and implements the most successful one.\nSummarizing these goals, we want to:\n\nCreate a machine learning model to predict which passengers survive\nHave excellent performance (to be clarified later)\nDevelop well-engineered, meaningful features\nUtilize cross-validation to select the best-performing model\n\n\n\nFormulate a Project Plan\nWhat kind of technologies do we want to use, and how will we implement them?\nWe’ll need to revisit this as we get further understanding of the data, but initially, we could make use of:\n\nLogistic Regression\nNaive-Bayes\nk-Nearest Neighbors\nDecision Tree\nRandom Forest\nGradient-Boosted Trees\nAda Boost\nSupport Vector Machines\n\nMuch of these are quickly available in the Scikit-Learn library. In addition, this library also provides tools for:\n\nTrain-Test splits\nK-fold cross-validation\nStratified K-Fold cross-validation\nGrid search CV\nAccuracy metrics"
  },
  {
    "objectID": "projects/titanic/index.html#data-understanding",
    "href": "projects/titanic/index.html#data-understanding",
    "title": "Titanic",
    "section": "2. Data Understanding",
    "text": "2. Data Understanding\nThis stage is more commonly referred to as EDA, or exploratory data analysis. We want to import our data and understand its structure, what problems we need to fix, and any other factors that may affect our workflow.\n\nLoad Data\n\n# Import required modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.axes import Axes\nimport seaborn as sns\n\n\ndf = pd.read_csv('data/Titanic-Dataset.csv')\nprint('Loaded data into variable `df`')\n\nLoaded data into variable `df`\n\n\n\n\nDescribe our Data\nHere, we want to understand some high-level characteristics of our data, namely:\n\nData preview (i.e., the head)\nData types\nDescriptive statistics\nNull value distribution\n\n\n# Display first 10 rows\ndf.head(10)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n0\n1\n0\n3\nBraund, Mr. Owen Harris\nmale\n22.0\n1\n0\nA/5 21171\n7.2500\nNaN\nS\n\n\n1\n2\n1\n1\nCumings, Mrs. John Bradley (Florence Briggs Th...\nfemale\n38.0\n1\n0\nPC 17599\n71.2833\nC85\nC\n\n\n2\n3\n1\n3\nHeikkinen, Miss. Laina\nfemale\n26.0\n0\n0\nSTON/O2. 3101282\n7.9250\nNaN\nS\n\n\n3\n4\n1\n1\nFutrelle, Mrs. Jacques Heath (Lily May Peel)\nfemale\n35.0\n1\n0\n113803\n53.1000\nC123\nS\n\n\n4\n5\n0\n3\nAllen, Mr. William Henry\nmale\n35.0\n0\n0\n373450\n8.0500\nNaN\nS\n\n\n5\n6\n0\n3\nMoran, Mr. James\nmale\nNaN\n0\n0\n330877\n8.4583\nNaN\nQ\n\n\n6\n7\n0\n1\nMcCarthy, Mr. Timothy J\nmale\n54.0\n0\n0\n17463\n51.8625\nE46\nS\n\n\n7\n8\n0\n3\nPalsson, Master. Gosta Leonard\nmale\n2.0\n3\n1\n349909\n21.0750\nNaN\nS\n\n\n8\n9\n1\n3\nJohnson, Mrs. Oscar W (Elisabeth Vilhelmina Berg)\nfemale\n27.0\n0\n2\n347742\n11.1333\nNaN\nS\n\n\n9\n10\n1\n2\nNasser, Mrs. Nicholas (Adele Achem)\nfemale\n14.0\n1\n0\n237736\n30.0708\nNaN\nC\n\n\n\n\n\n\n\n\nNotes:\n\nPassengerId looks like a simple incrementing ID; this should be set as the index.\nSurvived is our target, a one-hot variable with 1 = “Survived”\nPClass seems to be a numeric value indicating first, second, or third class rooms on the ship.\nName doesn’t just include first/last name, it also includes title. This may be useful for an engineered feature.\nSex seems to be only male/female – this should be converted to a one-hot variable.\nAge - I see one NaN value here. I also notice it got represented as a float – is there a passenger whose age isn’t a whole integer?\nSibSp - Not immediately clear, but this one represents the number of siblings or spouses on the Titanic. I.e., other people in their group of the same age.\nParch - Like SibSp, this isn’t immediately clear, but the documentation says this represents the number of parents/children in their group. Both this column and SibSp may be useful to quantify the total people in the party’s group.\nTicket - Some of these are integers (ticket number), but some also have information like ‘A/5’, ‘PC’, ‘STON/O2’. Will need to dig further to understand this field.\nFare - How much was paid for the ticket. Do higher-paying customers have better chance of survival?\nCabin - The cabin number. May be useful if Pclass isn’t available.\nEmbarked - S = Southampton, C = Cherbourg, Q = Queenstown. Note that the order here is not arbitrary: This is the order of ports that the Titanic visited.\n\n\n\n# Display data types\ndisplay(df.dtypes.value_counts().to_frame('# Columns'))\ndisplay(df.dtypes.to_frame(name='Type'))\n\n\n\n\n\n\n\n\n# Columns\n\n\n\n\nint64\n5\n\n\nobject\n5\n\n\nfloat64\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nType\n\n\n\n\nPassengerId\nint64\n\n\nSurvived\nint64\n\n\nPclass\nint64\n\n\nName\nobject\n\n\nSex\nobject\n\n\nAge\nfloat64\n\n\nSibSp\nint64\n\n\nParch\nint64\n\n\nTicket\nobject\n\n\nFare\nfloat64\n\n\nCabin\nobject\n\n\nEmbarked\nobject\n\n\n\n\n\n\n\n\nNotes:\nAs mentioned above, it’s interesting that the Age field came in as a float. It may make more sense to convert this to an integer, unless that partial year gives us some kind of information.\nAlso, while it may be tempting to get rid of the 5 string fields, we can’t disregard this yet – we don’t have a ton of information to work with, so we’ll want to extract as much information as possible.\n\n\n# Describe the data\ndf.describe().round(2)\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nAge\nSibSp\nParch\nFare\n\n\n\n\ncount\n891.00\n891.00\n891.00\n714.00\n891.00\n891.00\n891.00\n\n\nmean\n446.00\n0.38\n2.31\n29.70\n0.52\n0.38\n32.20\n\n\nstd\n257.35\n0.49\n0.84\n14.53\n1.10\n0.81\n49.69\n\n\nmin\n1.00\n0.00\n1.00\n0.42\n0.00\n0.00\n0.00\n\n\n25%\n223.50\n0.00\n2.00\n20.12\n0.00\n0.00\n7.91\n\n\n50%\n446.00\n0.00\n3.00\n28.00\n0.00\n0.00\n14.45\n\n\n75%\n668.50\n1.00\n3.00\n38.00\n1.00\n0.00\n31.00\n\n\nmax\n891.00\n1.00\n3.00\n80.00\n8.00\n6.00\n512.33\n\n\n\n\n\n\n\n\nNotes:\n\nPassengerId - Irrelevant, as this is just an index.\nSurvived - The min/max don’t tell us much, but the mean is useful – 38% of passengers survived, so around 1 in 3.\nPclass - This is an ordinal field, not numeric, but it does tell us that the average passenger was between class 2 and 3.\nAge - There is a variety of ages on the Titanic, with the typical passenger being about 28-29 years of age, the youngest was 0.42 (5 months) and the eldest was 80 years old. Most passengers (25-75 percentile) were between 20-38 years old.\nSibSp - Most passengers (&gt;50%) didn’t have any siblings or spouses. Even the 75% percentile was just 1 other person of their age. The largest group was 8 siblings/spouses (almost certainly 8 siblings).\nParch - The vast majority (&gt;75%) didn’t have any parents or children aboard, but one group had 6 parents/children.\nFare - On average, passengers paid $32.20 with a wide standard deviation of $49.69. Most passengers paid between $8-31, with the wealthiest passenger paying a whopping $512.33 for their fare.\n\n\n\n# Understand the null distribution\nnull_distribution = pd.DataFrame({\n    'Null Count': df.isna().sum().map(lambda n: '' if n == 0 else f'{n:,}'),\n    '% Null': df.isna().mean().map(lambda p: '' if p == 0 else f'{p:.2%}')\n})\nnull_distribution = null_distribution.loc[null_distribution.iloc[:,0] != '']\n\nnull_distribution.T\n\n\n\n\n\n\n\n\nAge\nCabin\nEmbarked\n\n\n\n\nNull Count\n177\n687\n2\n\n\n% Null\n19.87%\n77.10%\n0.22%\n\n\n\n\n\n\n\n\nNotes:\nMost fields don’t have any null values, which is helpful. The most problematic field will be Cabin, but from looking at the top 10 rows, it doesn’t seem like this will be useful. We may just drop this field all together.\nAge is an interesting one: there is a significant amount of nulls here, but it may be quite a useful field. We could look at doing a quick predictive model based on Name, Fare, Embarked, SibSp, and Parch. Or, if we want something quick, we could do a simple median-value or mean-value imputation.\nEmbarked will probably require a quick manual fix here, since it’s only 2 records.\n\n\n\nExplore the Data\nNow for the fun part of EDA–the visuals! Let’s try to visualize what our data looks like. Each cell will be looking to answer a preliminary question or one raised in the data description step prior to this.\n\ndef add_bar_labels(\n        ax: Axes,\n        fmt: str,\n        label_type='center',\n        color='white'\n) -&gt; None:\n    for container in ax.containers:\n        ax.bar_label(container, label_type=label_type, fmt=fmt, color=color)\n    return\n\n\n# How did class play a role in survival?\ndf_class = df.groupby('Pclass', as_index=False)[['Survived']].mean()\nax = df_class.plot(\n    kind='bar',\n    x='Pclass',\n    y='Survived',\n    xlabel='Passenger Class',\n    ylabel='Passenger Survival (%)',\n    rot=0,\n    color='dodgerblue',\n    legend=False,\n    title='Passenger Survival by Class'\n)\n\n# X Labels\nax.set_xticks(range(3), ['First', 'Second', 'Third'])\n\n# Y Labels\nyticks = ax.get_yticks()\nytick_labels = map('{:.0%}'.format, yticks)\nax.set_yticks(yticks, ytick_labels)\n\nadd_bar_labels(ax, fmt='{:.1%}')\n\n# ax.set_title('Passenger Survival by Class', size=14, pad=12)\nplt.show()\n\n\n\n\n\n\n\n\n\nNotes:\nUnsurprisingly, first-class passengers has the best chance of surviving, followed by second, then third class.\n\n\n# What was the distribution of sex on the Titanic? How did this affect surival?\n\n# Create pivot-table by sex\ndf_sex = df.groupby(['Sex', 'Survived'], as_index=False)[['PassengerId']].count()\ndf_sex = df_sex.pivot(index='Sex', columns='Survived', values='PassengerId')\ndf_sex.rename(inplace=True, columns=lambda s: ['No', 'Yes'][s])\ndf_sex.sort_index(inplace=True, axis=1, ascending=False)  # Show 'Yes' first\n\n# Plot data\nax = df_sex.plot(\n    kind='bar',\n    stacked=True,\n    color=['dodgerblue', 'lightgray'],\n    legend=False,\n    title='Survival by Sex',\n    rot=0,\n)\nadd_bar_labels(ax, fmt='{:,.0f}', color='black')\nplt.show()\n\n\n\n\n\n\n\n\n\n# How does Age play an impact in survival?\ndf_age = df.groupby('Age', as_index=False)[['Survived']].mean()\ndf_age['Survived'] = df_age['Survived'].ewm(alpha=0.2).mean()\n\nax = df_age.plot(\n    kind='line',\n    x='Age',\n    y='Survived',\n    legend=False,\n    title='Survival by Age',\n    xlabel='Age',\n    ylabel='Survival (%)',\n    xlim=(0, 80),\n    ylim=(0, 1),\n)\n\nyticks = ax.get_yticks()\nytick_labels = map('{:.0%}'.format, yticks)\nax.set_yticks(yticks, ytick_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\n# Did older passengers pay higher fares?\n\ndf_age_fare = df.groupby('Age', as_index=False)[['Fare']].mean()\ndf_age_fare['Fare'] = df_age_fare['Fare'].ewm(alpha=0.05).mean()\n\nax = df_age_fare.plot(\n    kind='line',\n    x='Age',\n    y='Fare',\n    legend=False,\n    title='Fare by Age',\n    xlabel='Age',\n    ylabel='Average Fare ($)',\n    xlim=(1, 80),\n    ylim=(0, 60),\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n# How does Fare play an impact in survival?\ndf_fare = df.groupby('Fare', as_index=False)[['Survived']].mean()\ndf_fare['Survived'] = df_fare['Survived'].ewm(alpha=0.02).mean()\n\nax = df_fare.plot(\n    kind='line',\n    x='Fare',\n    y='Survived',\n    legend=False,\n    title='Survival by fare',\n    xlabel='Fare',\n    ylabel='Survival (%)',\n    xlim=(0, 500),\n    ylim=(0, 1),\n)\n\nyticks = ax.get_yticks()\nytick_labels = map('{:.0%}'.format, yticks)\nax.set_yticks(yticks, ytick_labels)\nplt.show()\n\n\n\n\n\n\n\n\n\nNotes:\nIt seems that generally, higher fares meant higher chance of survival, but there was a small dip around $30-50 fare, where passnegers that paid around $20-30 had a slightly higher chance of survival.\n\n\n# What are the most common words within \"Name\"?\nis_alnum_or_space = lambda s: s.isalpha() or s == ' '\n\nname_words = str.split(' '.join(\n    df['Name'].map(lambda name: ''.join(filter(is_alnum_or_space, name)))\n))\nword_counts = pd.Series(name_words).value_counts().reset_index()\nword_counts.columns = ['Word', 'Count']\n\nax = word_counts.head(10).plot(\n    kind='barh',\n    x='Word',\n    y='Count',\n)\n\nplt.show()\n\n\n\n\n\n\n\n\n\nNote:\nIt doesn’t seem like this extracts much information, since common names like “William”, “John”, and “Henry” are captured as common words in names. But, we may be able to extract the person’s title from the structure of the name: nearly every name is structured as “LastName, Title. FirstName (Alt Name)”\n\n\n# What are the most common titles?\ndef extract_name_title(name: str) -&gt; str:\n    '''\n    Get a person's title out of their name.\n    '''\n    title_and_firstname = name.split(', ')[1]\n    title = title_and_firstname.split('. ')[0]\n    return title\n\n\nname_titles = df['Name'].map(extract_name_title).value_counts().to_frame()\nname_titles = name_titles.reset_index()\nname_titles.columns = ['Title', 'Count']\nname_titles = name_titles.sort_values(by='Count', ascending=True)\nax = name_titles.plot(\n    kind='barh',\n    x='Title',\n    y='Count',\n    xlim=(0, 600),\n    title='Titles Within Passenger Names',\n    legend=False,\n    xlabel='Count',\n    ylabel='Title',\n)\nadd_bar_labels(ax, fmt='{:.0f}', label_type='edge', color='black')\nplt.show()\n\n\n\n\n\n\n\n\n\n# How strong is the correlation between variables?\nax = sns.heatmap(df.corr(numeric_only=True), cmap='RdYlGn', annot=True)\nax.set_title('Feature Correlation')\nplt.show()\n\n\n\n\n\n\n\n\n\n# What were the most common cabins?\ndf \\\n    .loc[~df.Cabin.isna(), 'Cabin'] \\\n    .map(lambda s: s[0]) \\\n    .value_counts() \\\n    .sort_index(ascending=False) \\\n    .reset_index() \\\n    .plot(\n        kind='barh',\n        x='Cabin',\n        y='count',\n        xlim=(0, 70),\n        title='Distribution of Passengers Across Decks',\n        legend=False,\n        xlabel='Count',\n        ylabel='Cabin',\n    )\nadd_bar_labels(ax, fmt='{:.0f}', label_type='edge', color='black')\n\nplt.show()"
  },
  {
    "objectID": "projects/titanic/index.html#data-preparation",
    "href": "projects/titanic/index.html#data-preparation",
    "title": "Titanic",
    "section": "3. Data Preparation",
    "text": "3. Data Preparation\nData scientists love to remind each other that roughly 80% of the project involves data preparation. It’s a good rule of thumb, but ultimately is meant to remind us that we should take extra time to synthesize our data.\nSome steps we will take:\n\nClean Data\nTransform Data\n\nTypically, the Transform step is done in 4 parts: Select, Construct, Integrate, and Format. For our purposes, we won’t need to integrate with any additional sources, other than some minor details for the Embarked field. Data formatting is covered pretty thoroughly in the Cleaning stage, so the main step left is the selection of useful features and construction by means of transformation/encoding.\n\nClean Data\n\nPlan\nMost of our data is pretty clean for what we need. There are just some null values that need filling:\n\n\n\nColumn\nStrategy\n\n\n\n\nAge\nMedian-Value Fill\n\n\nCabin\nDefault-Value Fill\n\n\nEmbarked\nBackground Research (2 values)\n\n\n\n\ndf_clean = df.copy()   # Preserve raw data\n\n\n\nAge: Median-Value Fill\nTo balance simplicity of analysis with as beneficial of an imputation as possible, we can look to use the median age by Sex and Pclass to fill the null values of Age.\n\ndef median_value_fill(\n        df: pd.DataFrame,\n        column: str,\n        group_by: str | list[str],\n) -&gt; pd.DataFrame:\n    median_column = f'Median{column}'\n    median_values = df \\\n        .groupby(group_by)[column].median() \\\n        .reset_index() \\\n        .rename(columns={column: median_column})\n    df = df.merge(median_values)\n    df[column] = df[column].fillna(df[median_column])\n    df = df.drop(median_column, axis=1)\n    return df\n\n\ndf_clean = df_clean.pipe(median_value_fill, 'Age', ['Sex', 'Pclass'])\n\n\n\nCabin: Default-Value Fill\n\ndf_clean['Cabin'] = df_clean.Cabin.fillna('Unknown')\n\n\n\nEmbarked: Background Research\n\ndf.loc[df.Embarked.isna()]\n\n\n\n\n\n\n\n\nPassengerId\nSurvived\nPclass\nName\nSex\nAge\nSibSp\nParch\nTicket\nFare\nCabin\nEmbarked\n\n\n\n\n61\n62\n1\n1\nIcard, Miss. Amelie\nfemale\n38.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n829\n830\n1\n1\nStone, Mrs. George Nelson (Martha Evelyn)\nfemale\n62.0\n0\n0\n113572\n80.0\nB28\nNaN\n\n\n\n\n\n\n\nThere are only 2 passengers who are missing the Embarked field, both with first-class tickets:\n\nMiss (Rose) Amelie Icard, age 38.\nMrs. Martha Evelyn Stone, age 62.\n\nSince there are only 2 passengers without this field, it makes sense to do a little extra research to find where they embarked from. There is an excellent site called Encyclopedia Titanica that has information on many passengers. It helps that both of these passengers survived to tell their story.\nFrom some quick research, we can see that both Amelie and Martha boarded from Southampton, so we can fill their missing values with ‘S’.\nLastly, to help with the interpretation of feature importance, we can change the Embarked field to the full name. This will make it easier to read later, since ‘Southampton’ is faster to interpret than just ‘S’.\n\n\ndf_clean['Embarked'] = df_clean.Embarked.fillna('S')\n\nAside: A Reminder of the Human Stories Behind the Data\nThese 2 passengers were actually in the same group: Amelie boarded the Titanic as the maid to Martha Evelyn Stone. Encyclopedia Titanica gives a brief story of their experience when the Titanic struck the iceberg:\n\nMrs Stone boarded the Titanic in Southampton on 10 April 1912 and was traveling in first class with her maid Amelie Icard. She occupied cabin B-28.\n\n\nMartha was awake in bed when the Titanic struck the iceberg. She slipped a kimono over her night dress, put on her slippers, and went out into the corridor and found other people similarly attired. She asked a crew member if they had struck an iceberg. “Yes,” he said, “but there is no danger. Go back to bed and to sleep.” At this time, Mrs Stone could hear the roar of the steam blowing off and she asked the officer why they were doing this. He told her they had stopped to see what damage there was and that there wasn’t any danger.\n\n\nShe went back to bed and never received a warning. The roaring steam went on for what seemed like forever so she got up and dressed and stepped out into the corridor. There, the daughter of the woman across the hall came running down the corridor, telling her to put on her life preserver and that they must get into the boats. Stone hurried to deck with the woman. They found the sailors getting into the lifeboats, but that there was no real order in loading the boats.\n\n\nStone and her maid got into lifeboat 6 and were rescued. She thought there were about 20 women and two men in the boat. Her role in the boat was to stand on the plug, which she did for seven hours. Another woman waved the only lantern they had in the boat for seven hours. Mrs Stone was sharply critical of how the Titanic crew handled the dilemma they faced that night.\n\n   \n\n\n\nTransform Data\n\nPlan\n\n\n\n\n\n\n\nColumn\nTransformations\n\n\n\n\nPassengerId\nSet as Index\n\n\nSurvived\nNone\n\n\nPclass\nOne-Hot Encoding\n\n\nName\nExtract Title\n\n\nSex\nOne-Hot Encoding\n\n\nAge\nOrdinal Encoding -&gt; One-Hot Encoding\n\n\nSibSp, Parch\nExtract FamilySize -&gt; Ordinal Encoding -&gt; One-Hot Encoding\n\n\nTicket\nDrop\n\n\nFare\nOrdinal Encoding -&gt; One-Hot Encoding\n\n\nCabin\nExtract Deck\n\n\nEmbarked\nOne-Hot Encoding\n\n\n\n\ndf_transformed = df_clean.copy()\n\n\n\nPassengerId: Set as Index\n\nif 'PassengerId' in df_transformed.columns:\n    df_transformed = df_transformed.set_index('PassengerId')\n\n\n\nPclass: One-Hot Encoding\n\nif 'Pclass' in df_transformed.columns:\n    pclass_map = {1: 'FirstClass', 2: 'SecondClass', 3: 'ThirdClass'}\n    df_transformed['Pclass'] = df_transformed.Pclass.map(pclass_map)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='',\n        prefix_sep='',\n        columns=['Pclass'],\n        dtype=int\n    )\n\n\n\nName: Extract Title\n\ndef get_binned_title_from_name(name: str) -&gt; str:\n    title_and_firstname = name.split(', ')[1]\n    title = title_and_firstname.split('. ')[0]\n    if title in ['Mr', 'Master']:\n        return 'MrMaster'\n    elif title in ['Miss', 'Mrs', 'Ms']:\n        return 'MissMrsMs'\n    else:\n        return 'Other'\n\n\nif 'Name' in df_transformed.columns:\n    df_transformed['Title'] = df_transformed['Name'] \\\n        .map(get_binned_title_from_name)\n    df_transformed = df_transformed.drop('Name', axis=1)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='Title',\n        prefix_sep='',\n        columns=['Title'],\n        dtype=int\n    )\n\n\n\nSex: One-Hot Encoding\n\nif 'Sex' in df_transformed.columns:\n    df_transformed['Male'] = df_transformed['Sex'].eq('male').astype(int)\n    df_transformed.drop('Sex', axis=1, inplace=True)\n\n\n\nAge: Ordinal Encoding -&gt; One-Hot Encoding\n\ndef apply_ordinal_to_one_hot(\n        df: pd.DataFrame,\n        column: str,\n        q: int\n) -&gt; pd.DataFrame:\n    df = df.copy()\n    categories = pd.qcut(df[column], q).cat.categories\n    labels = [\n        '{:.0f}To{:.0f}'.format(cat.left, cat.right)\n        for cat in categories\n    ]\n    df[column] = pd.qcut(df[column], q, labels)\n    return pd.get_dummies(df, columns=[column], prefix_sep='', dtype=int)\n\n\nif 'Age' in df_transformed.columns:\n    df_transformed = df_transformed.pipe(apply_ordinal_to_one_hot, 'Age', q=6)\n\n\n\nSibSp / Parch: Extract FamilySize -&gt; Ordinal Encoding -&gt; One-Hot Encoding\nIt’s likely that, while SibSp and Parch may not be useful alone, the combination gives us insight: the size of the family/group. We can also apply binning here to group passenger family sizes, whether they traveled alone (which was fairly common) or had a small, medium, or large family.\n\ndef encode_family_size(size: int) -&gt; str:\n    if size == 1:\n        return 'Alone'\n    elif size &lt;= 4:\n        return 'SmallFamily'\n    elif size &lt;= 6:\n        return 'MediumFamily'\n    else:\n        return 'LargeFamily'\n\n\nif 'SibSp' in df_transformed.columns:\n    cols = ['SibSp', 'Parch']\n    df_transformed['FamilySize'] = df_transformed[cols] \\\n        .sum(axis=1) \\\n        .add(1) \\\n        .map(encode_family_size)\n    df_transformed.drop(cols, axis=1, inplace=True)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='',\n        prefix_sep='',\n        columns=['FamilySize'],\n        dtype=int\n    )\n\n\n\nTicket: Drop\n\nif 'Ticket' in df_transformed.columns:\n    df_transformed.drop('Ticket', axis=1, inplace=True)\n\n\n\nFare: Ordinal Encoding -&gt; One-Hot Encoding\n\nif 'Fare' in df_transformed.columns:\n    df_transformed = df_transformed.pipe(apply_ordinal_to_one_hot, 'Fare', q=6)\n\n\ndf_transformed\n\n\n\n\n\n\n\n\nSurvived\nCabin\nEmbarked\nFirstClass\nSecondClass\nThirdClass\nTitleMissMrsMs\nTitleMrMaster\nTitleOther\nMale\n...\nAlone\nLargeFamily\nMediumFamily\nSmallFamily\nFare-0To8\nFare8To9\nFare9To14\nFare14To26\nFare26To52\nFare52To512\n\n\nPassengerId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\nUnknown\nS\n0\n0\n1\n0\n1\n0\n1\n...\n0\n0\n0\n1\n1\n0\n0\n0\n0\n0\n\n\n2\n1\nC85\nC\n1\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n3\n1\nUnknown\nS\n0\n0\n1\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n1\nC123\nS\n1\n0\n0\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n0\nUnknown\nS\n0\n0\n1\n0\n1\n0\n1\n...\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n887\n0\nUnknown\nS\n0\n1\n0\n0\n0\n1\n1\n...\n1\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n888\n1\nB42\nS\n1\n0\n0\n1\n0\n0\n0\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n889\n0\nUnknown\nS\n0\n0\n1\n1\n0\n0\n0\n...\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n\n\n890\n1\nC148\nC\n1\n0\n0\n0\n1\n0\n1\n...\n1\n0\n0\n0\n0\n0\n0\n0\n1\n0\n\n\n891\n0\nUnknown\nQ\n0\n0\n1\n0\n1\n0\n1\n...\n1\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\n\n891 rows × 26 columns\n\n\n\n\n\nCabin: Extract Deck\nWhile much of the cabin numbers are null, we can still extract some info from the available numbers. From some background research, we know a few things about the Titanic decks:\n\nDecks A, B, and C only contained first-class passengers.\nDecks D and E had any class passenger.\nDecks F and G only held second- and third-class passengers.\n\nThere is an odd deck: T. This passenger is first-class, so we can bucket him into the ‘A’ deck with other first-class passengers.\nWe can group these decks as ‘ABC’, ‘DE’, and ‘FG’, leaving ‘X’ for any unknown decks.\n\ndef get_deck_from_cabin(cabin) -&gt; str:\n    default_deck = 'X'\n\n    if pd.isna(cabin):\n        return default_deck\n    \n    deck = str(cabin)[0]\n    for bucket in ['ABC', 'DE', 'FG']:\n        if deck in bucket:\n            return bucket\n    else:\n        return default_deck\n\n\nif 'Cabin' in df_transformed.columns:\n    df_transformed['Deck'] = df_transformed.Cabin.map(get_deck_from_cabin)\n    df_transformed.drop('Cabin', axis=1, inplace=True)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='Deck',\n        prefix_sep='',\n        columns=['Deck'],\n        dtype=int\n    )\n\n\n\nEmbarked: One-Hot Encoding\n\nif 'Embarked' in df_transformed.columns:\n    cities_map = {'S': 'Southampton', 'C': 'Cherbourg', 'Q': 'Queenstown'}\n    df_transformed['Embarked'] = df_transformed.Embarked.map(cities_map)\n    df_transformed = pd.get_dummies(\n        df_transformed,\n        prefix='Embarked',\n        prefix_sep='',\n        columns=['Embarked'],\n        dtype=int\n    )\n\n\ndf_transformed.head()\n\n\n\n\n\n\n\n\nSurvived\nFirstClass\nSecondClass\nThirdClass\nTitleMissMrsMs\nTitleMrMaster\nTitleOther\nMale\nAge0To19\nAge19To24\n...\nFare14To26\nFare26To52\nFare52To512\nDeckABC\nDeckDE\nDeckFG\nDeckX\nEmbarkedCherbourg\nEmbarkedQueenstown\nEmbarkedSouthampton\n\n\nPassengerId\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n1\n0\n0\n0\n1\n0\n1\n0\n1\n0\n1\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n2\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n1\n0\n0\n\n\n3\n1\n0\n0\n1\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n4\n1\n1\n0\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n1\n1\n0\n0\n0\n0\n0\n1\n\n\n5\n0\n0\n0\n1\n0\n1\n0\n1\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n1\n\n\n\n\n5 rows × 31 columns\n\n\n\nLook how clean our data looks!"
  },
  {
    "objectID": "projects/titanic/index.html#modeling",
    "href": "projects/titanic/index.html#modeling",
    "title": "Titanic",
    "section": "4. Modeling",
    "text": "4. Modeling\nWe finally made it to the fun part, the modeling! CRISP-DM suggests to iterate model building and assessment until you strongly believe that you have found the best model. This can be accomplished in 4 stages:\n\nSelect modeling techniques\nGenerate test design\nBuild model\nAssess model\n\nThe plan is to use steps 1-2 to select several different models and setup the experiments, then apply steps 3-4 to each of the models to train/evaluate the performance of each.\n\nSelect modeling techniques\nI hinted at this earlier, but there are several models we can apply here:\n\nLogistic Regression\nNaive-Bayes\nk-Nearest Neighbors\nDecision Tree\nRandom Forest\nGradient-Boosted Trees\nAda Boost\nSupport Vector Machines\n\nWhile we could go as far as looking at neural networks to introduce more sophisticated feature interactions, this may be overkill. However, if we don’t get the performance we’d like out of these models, we can circle back to deep learning methods.\n\n\nGenerate test design\nAs typical with machine learning, we’ll need to split our data into training/ testing data. From there, we can apply cross-validation to each model to determine the most optimal parameters for the model. A summary of our approach can be seen below.\n\n\n\nFor measuring performance, we could look at an F1 score to account for precision/recall. However, since the overall survival rate isn’t significantly different than the non-survival rate, a simple accuracy measure should suffice here.\nFinally, to tune the hyperparameters, we can use a grid search to find the optimal parameters. Note that, as the diagram above shows, the hyperparameter tuning will be done with a validation set, leaving the test set for the final model evaluation.\nWe can store the model, grid search results, hyperparameters, and performance scores in a DataFrame for easy access.\n\n# Split train/test datasets\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\ntarget = 'Survived'\ny = df_transformed[target].copy()\nX = df_transformed.drop(target, axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=0\n)\n\n\n\nBuild model\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier\n)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Setup dataframe to store model information\nmodel_names = [\n    'AdaBoost',\n    'DecisionTree',\n    'NaiveBayes',\n    'GradientBoosted',\n    'KNN',\n    'LogisticRegression',\n    'RandomForest',\n    'SVC',\n]\ncolumns = ['Model', 'GridSearchResults', 'OptimalParams', 'Accuracy']\nmodels = pd.DataFrame(index=model_names, columns=columns)\nmodels\n\n\n\n\n\n\n\n\nModel\nGridSearchResults\nOptimalParams\nAccuracy\n\n\n\n\nAdaBoost\nNaN\nNaN\nNaN\nNaN\n\n\nDecisionTree\nNaN\nNaN\nNaN\nNaN\n\n\nNaiveBayes\nNaN\nNaN\nNaN\nNaN\n\n\nGradientBoosted\nNaN\nNaN\nNaN\nNaN\n\n\nKNN\nNaN\nNaN\nNaN\nNaN\n\n\nLogisticRegression\nNaN\nNaN\nNaN\nNaN\n\n\nRandomForest\nNaN\nNaN\nNaN\nNaN\n\n\nSVC\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\ndef apply_grid_search(estimator: BaseEstimator, params: dict) -&gt; pd.Series:\n    model = GridSearchCV(estimator, params, scoring='accuracy', n_jobs=-1)\n    model.fit(X_train, y_train)\n    accuracy = model.score(X_test, y_test)\n    return pd.Series(dict(\n        Model=model.best_estimator_,\n        GridSearchResults=pd.DataFrame(model.cv_results_),\n        OptimalParams=model.best_params_,\n        Accuracy=accuracy,\n    ))\n\n\nNaive-Bayes\n\nparams = {'var_smoothing': np.logspace(-12, -7, 6)}\nmodels.loc['NaiveBayes'] = apply_grid_search(GaussianNB(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['NaiveBayes', 'Accuracy']))\nmodels.loc['NaiveBayes', 'Model']\n\nAccuracy: 78.21%\n\n\nGaussianNB(var_smoothing=np.float64(1e-12))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNB?Documentation for GaussianNBiFitted\n        \n            \n                Parameters\n                \n\n\n\n\npriors \nNone\n\n\n\nvar_smoothing \nnp.float64(1e-12)\n\n\n\n\n            \n        \n    \n\n\n\n\nLogistic Regression\n\nparams = {\n    'penalty':      ['l2', 'l1'],\n    'C':            np.logspace(-3, 3, 7),\n    'solver':       ['liblinear', 'saga'],\n    'class_weight': [None, 'balanced'],\n}\nmodels.loc['LogisticRegression'] = apply_grid_search(LogisticRegression(),\n                                                     params)\nprint('Accuracy: {:.2%}'.format(models.loc['LogisticRegression', 'Accuracy']))\nmodels.loc['LogisticRegression', 'Model']\n\nAccuracy: 83.24%\n\n\nLogisticRegression(C=np.float64(1.0), penalty='l1', solver='liblinear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegression?Documentation for LogisticRegressioniFitted\n        \n            \n                Parameters\n                \n\n\n\n\npenalty \n'l1'\n\n\n\ndual \nFalse\n\n\n\ntol \n0.0001\n\n\n\nC \nnp.float64(1.0)\n\n\n\nfit_intercept \nTrue\n\n\n\nintercept_scaling \n1\n\n\n\nclass_weight \nNone\n\n\n\nrandom_state \nNone\n\n\n\nsolver \n'liblinear'\n\n\n\nmax_iter \n100\n\n\n\nmulti_class \n'deprecated'\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nl1_ratio \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nK-Nearest Neighbors\n\nparams = {\n    \"n_neighbors\": [3, 5, 7, 11, 15],\n    \"weights\":     [\"uniform\", \"distance\"],\n    \"p\":           [1, 2],\n}\nmodels.loc['KNN'] = apply_grid_search(KNeighborsClassifier(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['KNN', 'Accuracy']))\nmodels.loc['KNN', 'Model']\n\nAccuracy: 77.09%\n\n\nKNeighborsClassifier(n_neighbors=15, p=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.KNeighborsClassifier?Documentation for KNeighborsClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_neighbors \n15\n\n\n\nweights \n'uniform'\n\n\n\nalgorithm \n'auto'\n\n\n\nleaf_size \n30\n\n\n\np \n1\n\n\n\nmetric \n'minkowski'\n\n\n\nmetric_params \nNone\n\n\n\nn_jobs \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nDecisionTree\n\nparams = {\n    \"criterion\":         [\"gini\", \"entropy\", \"log_loss\"],\n    \"max_depth\":         [None, 5, 10, 20],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\":  [1, 2, 5],\n    \"class_weight\":      [None, \"balanced\"],\n}\nmodels.loc['DecisionTree'] = apply_grid_search(DecisionTreeClassifier(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['DecisionTree', 'Accuracy']))\nmodels.loc['DecisionTree', 'Model']\n\nAccuracy: 82.68%\n\n\nDecisionTreeClassifier(min_samples_leaf=5, min_samples_split=5)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n5\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nSupport Vector Classifier (SVC)\n\nparams = {\n    \"kernel\":       [\"linear\", \"rbf\"],\n    \"C\":            np.logspace(-2, 2, 5),\n    \"gamma\":        [\"scale\", \"auto\"],\n    \"class_weight\": [None, \"balanced\"],\n}\nmodels.loc['SVC'] = apply_grid_search(SVC(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['SVC', 'Accuracy']))\nmodels.loc['SVC', 'Model']\n\nAccuracy: 80.45%\n\n\nSVC(C=np.float64(1.0))In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVC?Documentation for SVCiFitted\n        \n            \n                Parameters\n                \n\n\n\n\nC \nnp.float64(1.0)\n\n\n\nkernel \n'rbf'\n\n\n\ndegree \n3\n\n\n\ngamma \n'scale'\n\n\n\ncoef0 \n0.0\n\n\n\nshrinking \nTrue\n\n\n\nprobability \nFalse\n\n\n\ntol \n0.001\n\n\n\ncache_size \n200\n\n\n\nclass_weight \nNone\n\n\n\nverbose \nFalse\n\n\n\nmax_iter \n-1\n\n\n\ndecision_function_shape \n'ovr'\n\n\n\nbreak_ties \nFalse\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nRandomForest\n\nparams = {\n    \"n_estimators\":     [200, 400],\n    \"max_depth\":        [None, 10, 20],\n    \"max_features\":     [\"sqrt\", \"log2\"],\n    \"min_samples_leaf\": [1, 2, 5],\n    \"class_weight\":     [None, \"balanced\"],\n}\nmodels.loc['RandomForest'] = apply_grid_search(RandomForestClassifier(), params)\nprint('Accuracy: {:.2%}'.format(models.loc['RandomForest', 'Accuracy']))\nmodels.loc['RandomForest', 'Model']\n\nAccuracy: 83.24%\n\n\nRandomForestClassifier(max_depth=10, min_samples_leaf=2, n_estimators=400)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifier?Documentation for RandomForestClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nn_estimators \n400\n\n\n\ncriterion \n'gini'\n\n\n\nmax_depth \n10\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n2\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \n'sqrt'\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nbootstrap \nTrue\n\n\n\noob_score \nFalse\n\n\n\nn_jobs \nNone\n\n\n\nrandom_state \nNone\n\n\n\nverbose \n0\n\n\n\nwarm_start \nFalse\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmax_samples \nNone\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nAda-Boosted Decision Tree\n\nparams = {\n    \"n_estimators\":  [100, 300, 600],\n    \"learning_rate\": [0.01, 0.1, 0.5, 1.0],\n    'algorithm':     ['SAMME']\n}\nestimator = AdaBoostClassifier(models.loc['DecisionTree', 'Model'])\nmodels.loc['AdaBoost'] = apply_grid_search(estimator, params)\nprint('Accuracy: {:.2%}'.format(models.loc['AdaBoost', 'Accuracy']))\nmodels.loc['AdaBoost', 'Model']\n\nc:\\programming\\repos\\website\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The parameter 'algorithm' is deprecated in 1.6 and has no effect. It will be removed in version 1.8.\n  warnings.warn(\n\n\nAccuracy: 83.80%\n\n\nAdaBoostClassifier(algorithm='SAMME',\n                   estimator=DecisionTreeClassifier(min_samples_leaf=5,\n                                                    min_samples_split=5),\n                   learning_rate=0.01, n_estimators=100)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.AdaBoostClassifier?Documentation for AdaBoostClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nestimator \nDecisionTreeC...mples_split=5)\n\n\n\nn_estimators \n100\n\n\n\nlearning_rate \n0.01\n\n\n\nalgorithm \n'SAMME'\n\n\n\nrandom_state \nNone\n\n\n\n\n            \n        \n    estimator: DecisionTreeClassifierDecisionTreeClassifier(min_samples_leaf=5, min_samples_split=5)DecisionTreeClassifier?Documentation for DecisionTreeClassifier\n        \n            \n                Parameters\n                \n\n\n\n\ncriterion \n'gini'\n\n\n\nsplitter \n'best'\n\n\n\nmax_depth \nNone\n\n\n\nmin_samples_split \n5\n\n\n\nmin_samples_leaf \n5\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_features \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\nclass_weight \nNone\n\n\n\nccp_alpha \n0.0\n\n\n\nmonotonic_cst \nNone\n\n\n\n\n            \n        \n    \n\n\n\n\nGradient-Boosted Tree\n\nparams = {\n    \"n_estimators\":  [100, 300],\n    \"learning_rate\": [0.01, 0.05, 0.1],\n    \"max_depth\":     [2, 3, 5],\n    \"subsample\":     [1.0, 0.8],\n    \"max_features\":  [None, \"sqrt\", \"log2\"],\n}\nestimator = GradientBoostingClassifier()\nmodels.loc['GradientBoosted'] = apply_grid_search(estimator, params)\nprint('Accuracy: {:.2%}'.format(models.loc['GradientBoosted', 'Accuracy']))\nmodels.loc['GradientBoosted', 'Model']\n\nAccuracy: 81.56%\n\n\nGradientBoostingClassifier(learning_rate=0.01, max_depth=5, max_features='sqrt',\n                           n_estimators=300)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingClassifier?Documentation for GradientBoostingClassifieriFitted\n        \n            \n                Parameters\n                \n\n\n\n\nloss \n'log_loss'\n\n\n\nlearning_rate \n0.01\n\n\n\nn_estimators \n300\n\n\n\nsubsample \n1.0\n\n\n\ncriterion \n'friedman_mse'\n\n\n\nmin_samples_split \n2\n\n\n\nmin_samples_leaf \n1\n\n\n\nmin_weight_fraction_leaf \n0.0\n\n\n\nmax_depth \n5\n\n\n\nmin_impurity_decrease \n0.0\n\n\n\ninit \nNone\n\n\n\nrandom_state \nNone\n\n\n\nmax_features \n'sqrt'\n\n\n\nverbose \n0\n\n\n\nmax_leaf_nodes \nNone\n\n\n\nwarm_start \nFalse\n\n\n\nvalidation_fraction \n0.1\n\n\n\nn_iter_no_change \nNone\n\n\n\ntol \n0.0001\n\n\n\nccp_alpha \n0.0\n\n\n\n\n            \n        \n    \n\n\n\n\n\nAssess model\n\nax = models['Accuracy'].sort_values().plot(kind='barh', title='Model Accuracy')\nadd_bar_labels(ax, '{:.2%}')\nplt.show()"
  },
  {
    "objectID": "projects/titanic/index.html#evaluation",
    "href": "projects/titanic/index.html#evaluation",
    "title": "Titanic",
    "section": "5. Evaluation",
    "text": "5. Evaluation\nThrough the Titanic data science workflow, several key insights emerged:\n\nSurvival Rates: First-class passengers and females had significantly higher survival rates. Fare and age also showed notable correlations with survival.\nFeature Engineering: Extracting titles from names and grouping family sizes improved model interpretability and performance.\nData Cleaning: Most columns were clean, with targeted imputation for missing ages and embarked locations. Cabin data was largely missing but deck extraction provided some value.\nModeling: Multiple models were evaluated using grid search and cross-validation. Ensemble methods (Random Forest, Gradient Boosted Trees, AdaBoost) generally outperformed simpler models.\nBest Models: The highest accuracy was achieved by ensemble tree-based models, confirming the value of engineered features and robust preprocessing.\n\nOverall, the workflow demonstrated the importance of thorough data understanding, careful cleaning, and feature engineering in building effective predictive models for Titanic survival.\n\nmodel_coefficients = pd.DataFrame({\n    'Feature': X.columns,\n    'Coefficient': models.loc['LogisticRegression', 'Model'].coef_[0]\n})\nmodel_coefficients.set_index('Feature', inplace=True)\n\nplt.figure(figsize=(3, 8))\nsns.heatmap(model_coefficients, cmap='RdYlGn')\nplt.title('Logistic Regression Coefficients')\nplt.show()"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html",
    "href": "posts/stochastic-gradient-descent/index.html",
    "title": "Stochastic Gradient Descent",
    "section": "",
    "text": "Gradient descent is one of the most fundamental tools in data science, as it opens the door to models that don’t have a closed-form solution. Even more, it sets the foundation for more complex optimization algorithms like momentum and Adam.\nAn issue that arises with gradient descent is the time it takes to calculate the gradients for larger datasets. In such cases, we may look to algorithms like stochastic gradient descent to speed up the training. Effectively, we are sacrificing some accuracy in the direction of the gradient for faster calculation time of the gradients.\nOne question that comes to mind for me: Is stochastic gradient descent worth the tradeoff of speed for randomness? How much better does it perform?\nLet’s test this ourselves. We can use the standard Iris dataset, with some additional modifications that we’ll get to in later sections."
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#setup",
    "href": "posts/stochastic-gradient-descent/index.html#setup",
    "title": "Stochastic Gradient Descent",
    "section": "Setup",
    "text": "Setup\nTo start, we’ll import the needed modules and load the dataset:\n\n# Load modules\nfrom __future__ import annotations\nimport time\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Setup type hints\nfrom typing import Optional\nfrom numpy.typing import NDArray\n\n# Load data\ndf = sns.load_dataset('iris')\ndf.info()\ndf.head(5)\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 150 entries, 0 to 149\nData columns (total 5 columns):\n #   Column        Non-Null Count  Dtype  \n---  ------        --------------  -----  \n 0   sepal_length  150 non-null    float64\n 1   sepal_width   150 non-null    float64\n 2   petal_length  150 non-null    float64\n 3   petal_width   150 non-null    float64\n 4   species       150 non-null    object \ndtypes: float64(4), object(1)\nmemory usage: 6.0+ KB\n\n\n\n\n\n\n\n\n\nsepal_length\nsepal_width\npetal_length\npetal_width\nspecies\n\n\n\n\n0\n5.1\n3.5\n1.4\n0.2\nsetosa\n\n\n1\n4.9\n3.0\n1.4\n0.2\nsetosa\n\n\n2\n4.7\n3.2\n1.3\n0.2\nsetosa\n\n\n3\n4.6\n3.1\n1.5\n0.2\nsetosa\n\n\n4\n5.0\n3.6\n1.4\n0.2\nsetosa"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#data-preparation",
    "href": "posts/stochastic-gradient-descent/index.html#data-preparation",
    "title": "Stochastic Gradient Descent",
    "section": "Data Preparation",
    "text": "Data Preparation\nTo make the experiment cleaner, I am going to make a few changes to the dataset. As a good general practice, I like to create helper functions for each step of my data preparation. This helps with debugging and makes the whole process much smoother, plus we can create a data preparation workflow and apply all steps at once.\nFirst, I want to apply standard scaling. This is typical, but will improve the speed of our training – no need to put any further odds against us. This is done by effectively converting our raw data into z-scores:\n\\[ x_{scaled} = \\frac{x - \\mu_x}{\\sigma_x} \\]\n\ndef standard_scale(data: pd.Series) -&gt; pd.Series:\n    return (data - data.mean()) / data.std()\n\nNext, I want to use one-hot encoding on the output labels. This means our model will be a “setosa predictor”, as opposed to a predictor for all 3 species.\n\ndef one_hot_encode(data: pd.Series, target: str) -&gt; pd.Series:\n    return data.eq(target).astype(int)\n\nFinally, I wan to apply bootstrap resampling on the whole dataset. The standard Iris dataset only has 150 samples, which is trivially fast to compute the gradient. To make the impact of gradient calculations clearer, I’ll use bootstrap resampling to expand our dataset to a new size of 100,000.\n\ndef bootstrap_resample(df: pd.DataFrame, size: int) -&gt; pd.DataFrame:\n    return df.sample(size, replace=True).reset_index(drop=True)\n\nAs a final step, we can separate our data into the feature matrix X and label vector y:\n\ndef separate_xy(df: pd.DataFrame, target_column: str) -&gt; tuple[NDArray, ...]:\n    X = df.drop(target_column, axis=1).values\n    y = df[target_column].values\n    return X, y\n\nWith all the preparation steps ready, we can create a full workflow to transform our raw data into a dataset ready for experimentation:\n\ndef prepare_data(df: pd.DataFrame, size: int) -&gt; tuple[NDArray, ...]:\n    df = df.copy()   # Preserve raw data\n    target_column = 'species'\n    target_species = 'setosa'\n\n    for column in df.columns:\n        if column == target_column:\n            df[column] = one_hot_encode(df[column], target_species)\n        else:\n            df[column] = standard_scale(df[column])\n\n    df = bootstrap_resample(df, size=size)\n    X, y = separate_xy(df, target_column)\n    return X, y"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#logistic-regression-model",
    "href": "posts/stochastic-gradient-descent/index.html#logistic-regression-model",
    "title": "Stochastic Gradient Descent",
    "section": "Logistic Regression Model",
    "text": "Logistic Regression Model\n\nThe term logistic regression primarily comes from the logistic function, also called the sigmoid function, \\(\\sigma(z)\\):\n\\[\n\\sigma (z) = \\frac{1}{1 + \\exp{(-z)}}\n\\]\nThis S-shaped curve provides a maps any real number to a value in between 0 and 1. Mathematically, we can express this as:\n\\[\n\\sigma : \\mathbb{R} \\rightarrow \\left[ 0, 1 \\right]\n\\]\nThis function has a huge benefit: If we used linear regression to model probabilities, we may end up with probabilities like \\(p = 527.68\\) or \\(p = -1\\), since linear regression puts no limit on the bounds of the output. If we pass the linear model through the sigmoid function, however, we can ensure that all output values are between 0 and 1, which we can then interpret as a probability.\n\ndef sigmoid(z: NDArray) -&gt; NDArray:\n    return 1 / (1 + np.exp(-z))\n\nTo keep our classification model simple, we will use a Linear Regression model, which builds on the linear regression model by passing it though the sigmoid function. More precisely:\n\\[\np(y = 1 | x) = \\sigma ( x^T \\theta )\n\\]\nwhere \\(p(y = 1 | x)\\) is the probability of class 1 given the input features \\(x\\), and \\(\\theta\\) is the vector of coefficients.\nNote that the coefficients also includes a bias (intercept) term as the first element, so we typically pad matrix \\(X\\) with ones in the first column, allowing us to simply perform matrix multiplication to get our predictions.\n\ndef predict(X: NDArray, theta: NDArray) -&gt; NDArray:\n    '''\n    Parameters\n    ----------\n    X : NDArray\n        Input features, shape: `(n, d)`\n    theta : NDArray\n        Coefficients, shape: `(d + 1,)`\n\n    Returns\n    -------\n    p : NDArray\n        Probabilities, shape: `(n,)`\n    '''\n    X_aug = np.c_[np.ones((X.shape[0], 1)), X]\n    z = X_aug @ theta.reshape(-1, 1)\n    p = sigmoid(z)\n    return p.flatten()"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#cross-entropy-loss",
    "href": "posts/stochastic-gradient-descent/index.html#cross-entropy-loss",
    "title": "Stochastic Gradient Descent",
    "section": "Cross-Entropy Loss",
    "text": "Cross-Entropy Loss\nOne of the most common losses used in logistic regression is cross-entropy:\n\\[\nL(\\theta) = - \\frac{1}{n} \\sum_{i=1}^n \\bigl(\n    y_i \\log{p_i} + (1 - y_i) \\log{(1 - p_i)}\n\\bigr)\n\\]\n\ndef cross_entropy(y: NDArray, p: NDArray) -&gt; np.number:\n    '''\n    Parameters\n    ----------\n    y : NDArray\n        Shape: `(n,)`\n    p : NDArray\n        Shape: `(n,)`\n    '''\n    y, p = y.flatten(), p.flatten()\n    entropy = - y * np.log(p) - (1 - y) * np.log(1 - p)\n    return np.mean(entropy)\n\nFor both batch and stochastic gradient descent, we will need the gradient of the loss function:\n\\[\n\\nabla L(\\theta) = - \\frac{1}{n} \\sum_{i=1}^n \\bigl(\n    (y_i - p_i) \\; x_i\n\\bigr)\n\\]\n\ndef gradient(X: NDArray, y: NDArray, p: NDArray) -&gt; np.number:\n    '''\n    Parameters\n    ----------\n    X : NDArray\n        Input features, shape: `(n, d)`\n    y : NDArray\n        Output labels, shape: `(n,)`\n    p : NDArray\n        Predictions, shape: `(n,)`\n\n    Returns\n    -------\n    grad : NDArray\n        Gradients, shape: `(d + 1,)`\n    '''\n    X_aug = np.c_[np.ones((X.shape[0], 1)), X]\n    diff = (y - p).reshape(-1, 1)\n    grad = np.mean(- X_aug * diff, axis=0)\n    return grad"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#batch-gradient-descent",
    "href": "posts/stochastic-gradient-descent/index.html#batch-gradient-descent",
    "title": "Stochastic Gradient Descent",
    "section": "Batch Gradient Descent",
    "text": "Batch Gradient Descent\n\nWhile linear regression can be performed using a single, closed-form expression, we aren’t as lucky with many other machine learning models. The best case is using an iterative training approach like gradient descent.\nWe start by initializing a random vector for \\(\\theta\\), then update iteratively:\n\\[\n\\theta^{(\\tau + 1)} = \\theta^{(\\tau)} - \\eta \\nabla L(\\theta^{(\\tau)})\n\\]\nWhere \\(\\tau\\) is the step index, \\(\\theta^{(\\tau)}\\) is the coefficient vector at step \\(\\tau\\), and \\(\\eta\\) is the learning rate. While we have the option to make \\(\\eta\\) a function of \\(\\tau\\)–i.e. allowing larger or smaller steps at each iteration–we will keep it constant here for simplicity.\n\n\n\n\n\n\nNote\n\n\n\nThe standard version of gradient descent is also called “batch” gradient descent. This naming is a little confusing, as it seems to imply that the gradient is calculated in batches, when it is actually calculated for all samples at once. A better way to think of this naming is that the gradient is calculated in a single “batch” for each iteration.\n\n\n\ndef batch_gradient_descent(\n        X: NDArray,\n        y: NDArray,\n        epochs: int,\n        learning_rate: float=0.05,\n) -&gt; tuple[NDArray, NDArray]:\n    '''\n    Perform batch gradient descent over a given number of epochs.\n\n    Parameters\n    ----------\n    X : NDArray\n        Input features, shape `(n, d)`\n    y : NDArray\n        Output labels, shape `(n, 1)`\n    epochs : int\n        The number of training epochs to perform.\n    learning_rate : float, default: 0.05\n        The learning rate to use for gradient descent.\n\n    Returns\n    -------\n    theta : NDArray\n        The bias and coefficients, shape `(d + 1,)`\n    losses : NDArray\n        The history of training losses\n    '''\n    theta = np.random.random(size=(X.shape[1] + 1,))\n    losses = np.zeros((epochs, ))\n\n    for epoch in range(epochs):\n        p = predict(X, theta)\n        losses[epoch] = cross_entropy(y, p)\n        grad = gradient(X, y, p)\n        theta = theta - learning_rate * grad\n\n    return theta, losses"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#stochastic-gradient-descent",
    "href": "posts/stochastic-gradient-descent/index.html#stochastic-gradient-descent",
    "title": "Stochastic Gradient Descent",
    "section": "Stochastic Gradient Descent",
    "text": "Stochastic Gradient Descent\nAs is our objective to test, there are often situations where calculating the entire \\(\\nabla L(\\theta)\\) isn’t practical due to the time it takes to calculate. In these cases, we can use a modified version of gradient descent called stochastic gradient descent. Instead of using the entire training set to calculate the gradient, we take a smaller batch of samples and use those instead.\nIf we define an indexing set \\(\\mathbb{I} = \\{ 1, 2, \\dots, n \\}\\), then we can take a batch of samples \\(B \\subset \\mathbb{I}\\), where \\(\\left| B \\right| &lt; n\\). Our gradient equation then becomes:\n\\[\n\\nabla L(\\theta) = - \\frac{1}{\\left| B \\right|} \\sum_{i \\in B} \\bigl(\n    (y_i - p_i) \\; x_i\n\\bigr)\n\\]\n\ndef get_batched_data(\n        X: NDArray,\n        y: NDArray,\n        size: int\n) -&gt; tuple[NDArray, NDArray]:\n    '''\n    Parameters\n    ----------\n    X : NDArray\n        The input features, shape: `(n, d)`\n    y : NDArray\n        The output labels, shape: `(n, )`\n    size : int\n        The batch size\n    '''\n    full_indexes = np.arange(X.shape[0])\n    batch_indexes = np.random.choice(full_indexes, size=size)\n    return X[batch_indexes], y[batch_indexes]\n\nOutside of using batches instead of the full batch, our process for stochastic gradient descent is almost identical to batched gradient descent. However, to ensure we’re training to the same level between batch and stochastic gradient descent, we’ll add an additional, optional parameter for early stopping, measured by the training loss. If we provide this parameter, stop training and return the parameters. Otherwise, we’ll stop when the number of epochs is reached.\n\ndef stochastic_gradient_descent(\n        X: NDArray,\n        y: NDArray,\n        epochs: int=50,\n        learning_rate: float=0.05,\n        batch_size: int=32,\n        early_stopping_loss: Optional[float]=None\n) -&gt; tuple[NDArray, NDArray]:\n    '''\n    Perform batch gradient descent over a given number of epochs.\n\n    Parameters\n    ----------\n    X : NDArray\n        Input features, shape `(n, d)`\n    y : NDArray\n        Output labels, shape `(n, 1)`\n    epochs : int\n        The number of training epochs to perform.\n    learning_rate : float, default: 0.05\n        The learning rate to use for gradient descent.\n    batch_size : int, default: 32\n        The size of each batch\n    early_stopping_loss : float | None, default: None\n        The loss threshold to stop training. If this is never reached, will\n        train for the full number of epochs (default).\n\n    Returns\n    -------\n    theta : NDArray\n        The bias and coefficients, shape `(d + 1,)`\n    losses : NDArray\n        The history of training losses\n    '''\n    theta = np.random.random(size=(X.shape[1] + 1,))\n    losses = np.zeros((epochs, ))\n\n    if early_stopping_loss is None:\n        early_stopping_loss = -float('inf')\n\n    for epoch in range(epochs):\n        X_b, y_b = get_batched_data(X, y, batch_size)\n        p = predict(X_b, theta)\n        losses[epoch] = cross_entropy(y_b, p)\n\n        if losses[epoch] &lt;= early_stopping_loss:\n            return theta, losses\n        else:\n            grad = gradient(X_b, y_b, p)\n            theta = theta - learning_rate * grad\n\n    return theta, losses"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#experiment-setup",
    "href": "posts/stochastic-gradient-descent/index.html#experiment-setup",
    "title": "Stochastic Gradient Descent",
    "section": "Experiment Setup",
    "text": "Experiment Setup\nNow we have all the tools to setup an experiment. The main item to be careful of is ensuring that stochastic gradient descent reaches the same loss as batch gradient descent. Otherwise, we may be training for a shorter time but receiving a poorer model.\n\ndef time_gradient_descent(\n        sizes: list[int],\n        epochs: int=50,\n        learning_rate: float=0.05,\n        batch_size: int=32,\n) -&gt; pd.DataFrame:\n    times = {'Size': sizes, 'Batch': [], 'Stochastic': []}\n    for size in sizes:\n        X, y = prepare_data(df, size)\n        \n        # Batch Gradient Descent\n        t_start = time.perf_counter()\n        _, batch_losses = batch_gradient_descent(\n            X, y,\n            epochs=epochs,\n            learning_rate=learning_rate,\n        )\n        t_end = time.perf_counter()\n        times['Batch'].append(t_end - t_start)\n\n        # Stochastic Gradient Descent\n        t_start = time.perf_counter()\n        _ = stochastic_gradient_descent(\n            X, y,\n            epochs=10_000,\n            learning_rate=learning_rate,\n            batch_size=batch_size,\n            early_stopping_loss=np.min(batch_losses)\n        )\n        t_end = time.perf_counter()\n        times['Stochastic'].append(t_end - t_start)\n    \n    return pd.DataFrame(times)\n\nTo add additional stability, we can run our experiment several times and take the mean of the results:\n\nsizes = list(range(1000, 50000 + 1, 1000))\nn_simulations = 20\ndataframes = []\n\nfor _ in range(n_simulations):\n    dataframes.append(time_gradient_descent(sizes))\n\ntimes_data = pd.concat(dataframes).groupby('Size', as_index=False).mean()\n\nFinally, let’s see how the algorithms performed!\n\nfig, ax = plt.subplots(figsize=(12, 8))\ntimes_data.plot(x='Size', y='Batch', ax=ax, logy=True)\ntimes_data.plot(x='Size', y='Stochastic', ax=ax, logy=True)\nax.set_xlabel('Number of Samples')\nax.set_ylabel('Training Time (ms)')\nax.set_xlim(0)\nax.legend()\nax.set_title('Training Time of Batch vs. Stochastic Gradient Descent')\nplt.show()"
  },
  {
    "objectID": "posts/stochastic-gradient-descent/index.html#conclusion",
    "href": "posts/stochastic-gradient-descent/index.html#conclusion",
    "title": "Stochastic Gradient Descent",
    "section": "Conclusion",
    "text": "Conclusion\nIt’s quite incredible that stochastic gradient descent ran this much faster than batch gradient descent. Originally, I had expected to see an optimal point: a crossing of the two curves that indicates the point where batch gradient descent isn’t worth it and stochastic gradient descent becomes a better option. Instead, it looks like nearly always, stochastic gradient descent takes the cake.\nThe most peculiar finding is the sudden jump in training time for batch gradient descent, at around 26,000 samples. This baffled me for a while, as I expected a smooth transition of training time as a function of sample size. However, after re-running this several times, I kept getting the same result. After a bit of research, I found some enlightening information.\nThe jump in training time for batch gradient descent can likely be attributed to memory allocation. Up through around 26k rows, the arrays neatly fit into our CPU cache (L2/L3). At around 27k rows, the matrix-vector multiplications no longer fully fit in the cache, forcing far more frequent main memory access. This produced a sudden slowdown, requiring a change in the backend algorithm, ultimately affecting the performance.\nWhile I started this with the intent of understanding batch gradient descent, it also provided an insightful lesson in the backend nuances of Numpy."
  }
]